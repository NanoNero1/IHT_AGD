{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U2DWonNx69r"
   },
   "source": [
    "### AUTHOR: Dimitri Kachler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEAgjIot_r8h"
   },
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tny4XuKHB4VZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fTxMQlrI_rqG"
   },
   "outputs": [],
   "source": [
    "#User-Dependent Variables\n",
    "layerByLayer = False\n",
    "datasetChoice = \"MNIST\"\n",
    "\n",
    "# -------------- INACTIVE\n",
    "#useNeptune = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okBTEBAuZX54"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4xMEbdjEXUk3"
   },
   "outputs": [],
   "source": [
    "# Neural Networks\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch\n",
    "\n",
    "# Arrays & Mathematics\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#System / IO\n",
    "import abc\n",
    "import itertools\n",
    "import importlib\n",
    "\n",
    "#Data Visualization\n",
    "#import seaborn as sns\n",
    "\n",
    "#External Utilities\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eUqt83pkPVh",
    "outputId": "c825348b-ff86-4cfa-a182-ed7f773d78fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# CUDA Check\n",
    "print(torch.__version__)\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nwmhYoV2jqci"
   },
   "outputs": [],
   "source": [
    "# NOTE: you can still run this and it should still work and send the data to my Neptune.ai project,\n",
    "# unfortunately you won't be able to see the graph without my account\n",
    "# Capture makes it so that the cell doesn't output text\n",
    "# NOTE: %%capture doesn't work on jupyterlab\n",
    "#%%capture\n",
    "#\n",
    "try:\n",
    "    import neptune\n",
    "except ImportError as e:\n",
    "    %pip install -U neptune\n",
    "    import neptune\n",
    "#import neptune\n",
    "from getpass import getpass\n",
    "\n",
    "project=\"dimitri-kachler-workspace/sanity-MNIST\"\n",
    "api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNWQxNDllOS04OGY1LTRjM2EtYTczZi0xNWI0NTRmZTA1OTEifQ==\"\n",
    "#project = neptune.init_project(api_token=api_token, project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bQtl6tN0vWb"
   },
   "source": [
    "# Github Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeWatvCl4aXy",
    "outputId": "5e41612f-42fc-4e91-a870-4a400e42a726"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'IHT_AGD' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NanoNero1/IHT_AGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LViCBrmwbiWW"
   },
   "outputs": [],
   "source": [
    "# NOTE: This might be very expenisive! - only keep active on prototype testing\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPb-BbT41iR2",
    "outputId": "9f385b63-0227-4f08-f2bd-f405e60a64ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "#%cd /content/IHT_AGD/\n",
    "\n",
    "# NOTE: for some reason, there is difficulty in what directory git pull should be called\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Dw1TZCfWVg9p"
   },
   "outputs": [],
   "source": [
    "# Data Collection\n",
    "import IHT_AGD.data_loaders.dataLoaders as dataLoaders\n",
    "datasetChoice = dataLoaders.datasetChoice\n",
    "train_loader = dataLoaders.train_loader\n",
    "test_loader = dataLoaders.test_loader\n",
    "abort() # I should have the data loaded locally!!!!\n",
    "\n",
    "# Just for debugging\n",
    "#import IHT_AGD.architectures.architect\n",
    "#IHT_AGD.architectures.architect.seeVariable\n",
    "\n",
    "# Neural Netwok Architecture\n",
    "from IHT_AGD.architectures.convNets import MNIST_convNet\n",
    "\n",
    "# Taining and Testing Functions\n",
    "from IHT_AGD.modelTrainTest.trainingMetrics import getTestAccuracy,getTestLoss\n",
    "from IHT_AGD.modelTrainTest.trainLoop import train\n",
    "\n",
    "# Optimizers (base, SGD, AGD, IHT, etc.)\n",
    "from IHT_AGD.optimizers.baseOptimizer import myOptimizer\n",
    "from IHT_AGD.optimizers.vanillaSGD import vanillaSGD\n",
    "from IHT_AGD.optimizers.ihtSGD import ihtSGD\n",
    "from IHT_AGD.optimizers.vanillaAGD import vanillaAGD\n",
    "from IHT_AGD.optimizers.ihtAGD import ihtAGD\n",
    "from IHT_AGD.optimizers.nativePytorchSGD import dimitriPytorchSGD\n",
    "\n",
    "# Visualization Functions\n",
    "from IHT_AGD.visualizationGraphs.plotting import plotMetric\n",
    "\n",
    "# Experiment Functions\n",
    "from IHT_AGD.experimentScaffolding.chooseOptimizer import chooseOptimizer\n",
    "from IHT_AGD.experimentScaffolding.chooseOptimizer import fixedChooseOptimizer\n",
    "from IHT_AGD.experimentScaffolding.experimentFuncs import runOneExperiment\n",
    "from IHT_AGD.experimentScaffolding.experimentFuncs import runMainExperiment\n",
    "from IHT_AGD.experimentScaffolding.experimentFuncs import runPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVWidDypEprA",
    "outputId": "e5ba04df-4347-4adc-a7aa-925de5d5e8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "#To know the sizes\n",
    "firstInput, firstTarget = next(iter(train_loader))\n",
    "print(firstInput.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SoV-EkH4rie"
   },
   "source": [
    "# Experiment Github Imports?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwvZ1FqKdtjt"
   },
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9L2J1E7ifJ-7"
   },
   "outputs": [],
   "source": [
    "variablesToTrack = ['sparsity','sparsityBias','lr','iteration','trackSparsity','trackSparsityBias','trackSparsityLinear','testAccuracy','beta']\n",
    "expensiveVariables = ['testAccuracy']\n",
    "functionsToHelpTrack = ['trackingSparsity','getTestAccuracy']\n",
    "expensiveFunctions = ['getTestAccuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaMmGdAu32BP"
   },
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PETuCM2Z33cY",
    "outputId": "96b3ebaf-df43-4e9f-ce1a-1662dfceaf90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new setups\n",
      "[{'setupID': 'ihtAGD_SP70', 'scheme': 'ihtAGD', 'sparsity': 0.7, 'kappa': 5.0, 'beta': 50.0}, {'setupID': 'ihtAGD_SP90', 'scheme': 'ihtAGD', 'sparsity': 0.9, 'kappa': 5.0, 'beta': 50.0}, {'setupID': 'ihtAGD_SP95', 'scheme': 'ihtAGD', 'sparsity': 0.95, 'kappa': 5.0, 'beta': 50.0}, {'setupID': 'ihtAGD_SP99', 'scheme': 'ihtAGD', 'sparsity': 0.99, 'kappa': 5.0, 'beta': 50.0}]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# NOTE: I think it might be useful to keep the setups here, at least for now since we change the settings often\n",
    "setup_ihtAGD = {\n",
    "    \"scheme\":\"ihtAGD\" ,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":5.0,\n",
    "    \"beta\":50.0}\n",
    "\n",
    "setup_vanillaAGD = {\n",
    "    \"scheme\":\"vanillaAGD\",\n",
    "    \"kappa\":5.0,\n",
    "    \"beta\":50.0,\n",
    "    }\n",
    "\n",
    "setup_ihtSGD = {\n",
    "    \"scheme\":\"ihtSGD\" ,\n",
    "    \"sparsity\":0.900,\n",
    "    \"beta\": 50.0,}\n",
    "\n",
    "setup_vanillaSGD = {\n",
    "    \"scheme\":\"vanillaSGD\",\n",
    "    \"sparsity\":0.9,\n",
    "    \"beta\": 50.0,\n",
    "}\n",
    "\n",
    "setup_pytorchSGD = {\n",
    "    \"scheme\":\"pytorchSGD\"\n",
    "}\n",
    "\n",
    "setup_ihtAGD = {\n",
    "    \"scheme\":\"ihtAGD\" ,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":5.0,\n",
    "    \"beta\":50.0}\n",
    "\"\"\"\n",
    "print('new setups')\n",
    "\n",
    "experimentName = 'differentSparsities'\n",
    "setups = None\n",
    "exec(f\"import IHT_AGD.setups.setup_{experimentName}\")\n",
    "exec(f\"setups = IHT_AGD.setups.setup_{experimentName}.setups\")\n",
    "print(setups)\n",
    "#abort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q25ZdCrjzVZd"
   },
   "source": [
    "# Running the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gX9f4erHCDCE",
    "outputId": "b22b3f6b-a0fb-4f96-e614-2fdddaf0a09d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MNIST\n"
     ]
    }
   ],
   "source": [
    "print(datasetChoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TTG9HAmZRlwo",
    "outputId": "b62107f7-0e69-45db-c5ef-a7724758a9fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'setupID': 'ihtAGD_SP70', 'scheme': 'ihtAGD', 'sparsity': 0.7, 'kappa': 5.0, 'beta': 50.0}, {'setupID': 'ihtAGD_SP90', 'scheme': 'ihtAGD', 'sparsity': 0.9, 'kappa': 5.0, 'beta': 50.0}, {'setupID': 'ihtAGD_SP95', 'scheme': 'ihtAGD', 'sparsity': 0.95, 'kappa': 5.0, 'beta': 50.0}, {'setupID': 'ihtAGD_SP99', 'scheme': 'ihtAGD', 'sparsity': 0.99, 'kappa': 5.0, 'beta': 50.0}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/dimitri-kachler-workspace/sanity-MNIST/e/SAN-513\n",
      "this should print\n",
      "{'functionsToHelpTrack': ['trackingSparsity', 'getTestAccuracy'], 'variablesToTrack': ['sparsity', 'sparsityBias', 'lr', 'iteration', 'trackSparsity', 'trackSparsityBias', 'trackSparsityLinear', 'testAccuracy', 'beta'], 'expensiveVariables': ['testAccuracy'], 'expensiveFunctions': ['getTestAccuracy'], 'device': 'cuda', 'run': <neptune.metadata_containers.run.Run object at 0x00000220A9501D90>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x00000220A9500490>, 'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x00000220A950A650>}\n",
      "[{'epochs': 10, 'setupID': 'ihtAGD_SP70', 'scheme': 'ihtAGD', 'sparsity': 0.7, 'kappa': 5.0, 'beta': 50.0}, {'epochs': 10, 'setupID': 'ihtAGD_SP90', 'scheme': 'ihtAGD', 'sparsity': 0.9, 'kappa': 5.0, 'beta': 50.0}, {'epochs': 10, 'setupID': 'ihtAGD_SP95', 'scheme': 'ihtAGD', 'sparsity': 0.95, 'kappa': 5.0, 'beta': 50.0}, {'epochs': 10, 'setupID': 'ihtAGD_SP99', 'scheme': 'ihtAGD', 'sparsity': 0.99, 'kappa': 5.0, 'beta': 50.0}]\n",
      "0\n",
      "test fixed chooose\n",
      "{'model': MNIST_convNet(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "), 'scheme': 'ihtAGD', 'sparsity': 0.7, 'kappa': 5.0, 'beta': 50.0, 'functionsToHelpTrack': ['trackingSparsity', 'getTestAccuracy'], 'variablesToTrack': ['sparsity', 'sparsityBias', 'lr', 'iteration', 'trackSparsity', 'trackSparsityBias', 'trackSparsityLinear', 'testAccuracy', 'beta'], 'device': 'cuda', 'run': <neptune.metadata_containers.run.Run object at 0x00000220A9501D90>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x00000220A9500490>, 'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x00000220A950A650>, 'trialNumber': 0}\n",
      "Current Scheme: iht_AGD\n",
      "print loss:2.3040428161621094\n",
      "tensor(2.3040, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 0\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 60 / 80\n",
      "Iteration: 0\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.304043\n",
      "print loss:2.2904281616210938\n",
      "tensor(2.2904, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 1\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 61 / 80\n",
      "Iteration: 1\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:2.2481493949890137\n",
      "tensor(2.2481, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 2\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 62 / 80\n",
      "Iteration: 2\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:2.177140712738037\n",
      "tensor(2.1771, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 3\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 63 / 80\n",
      "Iteration: 3\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:2.087662696838379\n",
      "tensor(2.0877, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 4\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 64 / 80\n",
      "Iteration: 4\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:2.0069351196289062\n",
      "tensor(2.0069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 5\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 65 / 80\n",
      "Iteration: 5\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:1.887511968612671\n",
      "tensor(1.8875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 6\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 66 / 80\n",
      "Iteration: 6\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:1.7786521911621094\n",
      "tensor(1.7787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 7\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 67 / 80\n",
      "Iteration: 7\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:1.6443153619766235\n",
      "tensor(1.6443, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 8\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 68 / 80\n",
      "Iteration: 8\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:1.5179862976074219\n",
      "tensor(1.5180, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 9\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 69 / 80\n",
      "Iteration: 9\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:1.4019356966018677\n",
      "tensor(1.4019, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 10\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 70 / 80\n",
      "Iteration: 10\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.401936\n",
      "print loss:1.2773257493972778\n",
      "tensor(1.2773, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 11\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 71 / 80\n",
      "Iteration: 11\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:1.1744674444198608\n",
      "tensor(1.1745, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 12\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 72 / 80\n",
      "Iteration: 12\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:1.1296895742416382\n",
      "tensor(1.1297, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 13\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 73 / 80\n",
      "Iteration: 13\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:0.951042115688324\n",
      "tensor(0.9510, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 14\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 74 / 80\n",
      "Iteration: 14\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:0.8831204175949097\n",
      "tensor(0.8831, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 15\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 75 / 80\n",
      "Iteration: 15\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:0.8849450945854187\n",
      "tensor(0.8849, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 16\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 76 / 80\n",
      "Iteration: 16\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:0.7840753197669983\n",
      "tensor(0.7841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 17\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 77 / 80\n",
      "Iteration: 17\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:0.7525570392608643\n",
      "tensor(0.7526, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 18\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 78 / 80\n",
      "Iteration: 18\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:0.7317363023757935\n",
      "tensor(0.7317, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 19\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 79 / 80\n",
      "Iteration: 19\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "print loss:0.6984103918075562\n",
      "tensor(0.6984, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 20\n",
      "800\n",
      "torch.Size([32, 1, 5, 5])\n",
      "32\n",
      "torch.Size([32])\n",
      "589824\n",
      "torch.Size([128, 4608])\n",
      "128\n",
      "torch.Size([128])\n",
      "1280\n",
      "torch.Size([10, 128])\n",
      "10\n",
      "torch.Size([10])\n",
      "HowFarAlong: 0 / 80\n",
      "Iteration: 20\n",
      "warmup\n",
      "FIXED IHT-AGD\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.698410\n",
      "print loss:0.6133798956871033\n",
      "tensor(0.6134, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "speed iteration 21\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(setups)\n\u001b[0;32m      7\u001b[0m run \u001b[38;5;241m=\u001b[39m neptune\u001b[38;5;241m.\u001b[39minit_run(api_token\u001b[38;5;241m=\u001b[39mapi_token, project\u001b[38;5;241m=\u001b[39mproject)\n\u001b[1;32m----> 8\u001b[0m runPipeline(setups,\n\u001b[0;32m      9\u001b[0m             datasetChoice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m             epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m     11\u001b[0m             functionsToHelpTrack\u001b[38;5;241m=\u001b[39mfunctionsToHelpTrack,\n\u001b[0;32m     12\u001b[0m             variablesToTrack\u001b[38;5;241m=\u001b[39mvariablesToTrack,\n\u001b[0;32m     13\u001b[0m             expensiveVariables\u001b[38;5;241m=\u001b[39mexpensiveVariables,\n\u001b[0;32m     14\u001b[0m             expensiveFunctions\u001b[38;5;241m=\u001b[39mexpensiveFunctions,\n\u001b[0;32m     15\u001b[0m             \n\u001b[0;32m     16\u001b[0m             \n\u001b[0;32m     17\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[0;32m     18\u001b[0m             run\u001b[38;5;241m=\u001b[39mrun,\n\u001b[0;32m     19\u001b[0m             test_loader\u001b[38;5;241m=\u001b[39mtest_loader,\n\u001b[0;32m     20\u001b[0m             train_loader\u001b[38;5;241m=\u001b[39mtrain_loader)\n\u001b[0;32m     21\u001b[0m run\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\experimentScaffolding\\experimentFuncs.py:76\u001b[0m, in \u001b[0;36mrunPipeline\u001b[1;34m(setups, datasetChoice, epochs, trials, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m run[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetupDict\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mupload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msetups.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(trials):\n\u001b[1;32m---> 76\u001b[0m   runMainExperiment(setups,epochs\u001b[38;5;241m=\u001b[39mepochs,trialNumber\u001b[38;5;241m=\u001b[39mtrial,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\experimentScaffolding\\experimentFuncs.py:49\u001b[0m, in \u001b[0;36mrunMainExperiment\u001b[1;34m(setups, epochs, trialNumber, **kwargs)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(setups)):\n\u001b[0;32m     48\u001b[0m   \u001b[38;5;28mprint\u001b[39m(trialNumber)\n\u001b[1;32m---> 49\u001b[0m   all_models[idx] \u001b[38;5;241m=\u001b[39m runOneExperiment(setups[idx],trialNumber\u001b[38;5;241m=\u001b[39mtrialNumber,\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m all_models\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\experimentScaffolding\\experimentFuncs.py:32\u001b[0m, in \u001b[0;36mrunOneExperiment\u001b[1;34m(setup, trialNumber, datasetChoice, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, step_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, setup[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     28\u001b[0m \n\u001b[0;32m     29\u001b[0m   \u001b[38;5;66;03m#print(optimizer.methodName)\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \n\u001b[0;32m     31\u001b[0m   \u001b[38;5;66;03m# Call to run one epoch of training\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m   train([],model, kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m], kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loader\u001b[39m\u001b[38;5;124m'\u001b[39m], optimizer, epoch,trialNumber,run\u001b[38;5;241m=\u001b[39mkwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     34\u001b[0m   scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\modelTrainTest\\trainLoop.py:64\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(args, model, device, train_loader, optimizer, epoch, trialNumber, test_loader, run)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Optimization Step\u001b[39;00m\n\u001b[0;32m     63\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mcurrentDataBatch \u001b[38;5;241m=\u001b[39m (data,target)\n\u001b[1;32m---> 64\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m###LOG### On every 10 iterations, we can print out some information\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_idx \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m :\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:75\u001b[0m, in \u001b[0;36mLRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     74\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m---> 75\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrapped(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\optim\\optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m             )\n\u001b[1;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\optimizers\\ihtAGD.py:17\u001b[0m, in \u001b[0;36mihtAGD.step\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     16\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeed iteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 17\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogging()\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompressOrDecompress()\n\u001b[0;32m     19\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\optimizers\\baseOptimizer.py:46\u001b[0m, in \u001b[0;36mmyOptimizer.logging\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     44\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m function \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunctionsToHelpTrack:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m   \u001b[38;5;28meval\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m function \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Variables to Log\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m variable \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32m<string>:1\u001b[0m\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\optimizers\\baseOptimizer.py:61\u001b[0m, in \u001b[0;36mmyOptimizer.getTestAccuracy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# The testing accuracy is taken over the entire dataset\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest_loader:\n\u001b[0;32m     62\u001b[0m         data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice), target\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     63\u001b[0m         output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(data)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torchvision\\datasets\\mnist.py:146\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    143\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img\u001b[38;5;241m.\u001b[39mnumpy(), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 146\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(img)\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    149\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mto_tensor(pic)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\cudatest\\Lib\\site-packages\\torchvision\\transforms\\functional.py:168\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# handle PIL Image\u001b[39;00m\n\u001b[0;32m    167\u001b[0m mode_to_nptype \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint32, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mbyteorder \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlittle\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI;16B\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mint16, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mfloat32}\n\u001b[1;32m--> 168\u001b[0m img \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(pic, mode_to_nptype\u001b[38;5;241m.\u001b[39mget(pic\u001b[38;5;241m.\u001b[39mmode, np\u001b[38;5;241m.\u001b[39muint8), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pic\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    171\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m255\u001b[39m \u001b[38;5;241m*\u001b[39m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\" MAIN CELL \"\"\"\n",
    "#setups = [setup_ihtAGD]#,setup_vanillaSGD]#,setup_ihtAGD]\n",
    "#setups = [setup_pytorchSGD]\n",
    "print(setups)\n",
    "\n",
    "\n",
    "run = neptune.init_run(api_token=api_token, project=project)\n",
    "runPipeline(setups,\n",
    "            datasetChoice=\"MNIST\",\n",
    "            epochs=10,trials=5,\n",
    "            functionsToHelpTrack=functionsToHelpTrack,\n",
    "            variablesToTrack=variablesToTrack,\n",
    "            expensiveVariables=expensiveVariables,\n",
    "            expensiveFunctions=expensiveFunctions,\n",
    "            \n",
    "            \n",
    "            device=device,\n",
    "            run=run,\n",
    "            test_loader=test_loader,\n",
    "            train_loader=train_loader)\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zfWqas3i7d0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFXXFE2tYg-c"
   },
   "outputs": [],
   "source": [
    "%reload(runPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M0HjGxI2uK9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bSdr-W0lFkq"
   },
   "outputs": [],
   "source": [
    "importlib.reload(IHT_AGD.experimentScaffolding.experimentFuncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPnrL5azlFfC"
   },
   "outputs": [],
   "source": [
    "abort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-LBtmTsUiSM"
   },
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# END OF THE BASELINE FRAMEWORK, NEXT SECTION DEDICATED TO EXTENSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prSnPcFPUna-"
   },
   "source": [
    "## Bias Left Untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcAf_EsRUq0w"
   },
   "outputs": [],
   "source": [
    "class untouchedIhtAGD(ihtAGD):\n",
    "  def __init__(self,params,sparsity=0.9,kappa=5.0,beta=50.0):\n",
    "    super().__init__(params)\n",
    "    self.methodName = \"untouched_iht_AGD\"\n",
    "    self.alpha = beta / kappa\n",
    "    self.beta = beta\n",
    "    self.kappa = kappa\n",
    "\n",
    "  def sparsify(self):\n",
    "    # TO-DO: remember to remove this zero, it is inconsequential, but still remove it in good practice\n",
    "    concatWeights = torch.zeros((1)).to(device)\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "\n",
    "        #Skip Bias Layers\n",
    "        if len(p.data.shape) < 2:\n",
    "          continue\n",
    "\n",
    "        flatWeights = torch.flatten(p.data)\n",
    "        concatWeights = torch.cat((concatWeights,flatWeights),0)\n",
    "\n",
    "    topK = int(len(concatWeights)*(1-self.sparsity))\n",
    "    vals, bestI = torch.topk(torch.abs(concatWeights),topK,dim=0)\n",
    "    cutoff = vals[-1]\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "\n",
    "        #Skip Bias Layers\n",
    "        if len(p.data.shape) < 2:\n",
    "          continue\n",
    "\n",
    "        p.data[abs(p.data) <= cutoff] = 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS7mdfZIXedV"
   },
   "outputs": [],
   "source": [
    "setup_untouched_ihtAGD = {\n",
    "    \"scheme\":\"untouchedIhtAGD\",\n",
    "    \"lr\":0.1,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":10.0,\n",
    "    \"beta\":100.0}\n",
    "setups = [setup_untouched_ihtAGD, setup_ihtAGD]\n",
    "\n",
    "run = neptune.init_run(api_token=api_token, project=project)\n",
    "all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups)\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxXZNYnHQj9V"
   },
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_i4nU5QFQl3j"
   },
   "outputs": [],
   "source": [
    "from os import setgroups\n",
    "\n",
    "def gridSearch(default,variables,values,metric,epochs=1):\n",
    "  \"\"\" Desc: searches in a grid for the best combination of values of arbitrary dimension,\n",
    "        we can check for more than 2 variables at a time, but this can be very costly\n",
    "\n",
    "  default [dictionary]: a dictionary for all the default settings, this is also how one can set the type of algorithm\n",
    "  variables [array[string]]: the settings to change\n",
    "  values [2Darray]: what values to take on\n",
    "  metric [string]: what metric to use for the best value\n",
    "  \"\"\"\n",
    "\n",
    "  # We will not know how to traverse this list easily however\n",
    "  # TO-DO: find a way to organize, or traverse this list\n",
    "  setups = []\n",
    "\n",
    "  # This list has every possible combination of the settings\n",
    "  valuePermutations = list(itertools.product(*values))\n",
    "\n",
    "  for permutation in valuePermutations:\n",
    "    newSetup = default.copy()\n",
    "    for idx,val in enumerate(permutation):\n",
    "\n",
    "      # Adjusts the settings one-by-one\n",
    "      newSetup[variables[idx]] = val\n",
    "\n",
    "    setups.append(newSetup)\n",
    "\n",
    "  print(setups)\n",
    "\n",
    "\n",
    "  all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups,epochs=epochs)\n",
    "\n",
    "  # NEXT: Interchange with a different metric\n",
    "  # TO-DO: try \"highest loss\" over entire dataset using model\n",
    "\n",
    "  # Right now we use the accuracy in after the last epoch\n",
    "  # BUG: is the last epoch at 0 or -1 I need to check\n",
    "  min_accuracies = [accuracies[-1] for accuracies in all_accuracies]\n",
    "  bestSetupIndex = min_accuracies.index(min(min_accuracies))\n",
    "\n",
    "\n",
    "\n",
    "  return setups[bestSetupIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdkn9Z7tHt7y"
   },
   "outputs": [],
   "source": [
    "default = {\n",
    "    \"scheme\":\"vanillaAGD\",\n",
    "    \"lr\":0.1,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":15.0,\n",
    "    \"beta\":10000.0}\n",
    "# We set a big value to see if we overwrite it in the Grid Search\n",
    "\n",
    "gridSearch(default,[\"kappa\",\"beta\"],[[2.0,10.0,100.0],[10.0,100.0,300.0]],\"loss\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCz_DpeoYqoK"
   },
   "outputs": [],
   "source": [
    "#This works! It recognizes it as a class name\n",
    "type(eval(\"ihtAGD\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mTrAB1jIWvH"
   },
   "source": [
    "# **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BK4nfrXJNb14"
   },
   "source": [
    "# Saving and Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVY_NTm3OKeX"
   },
   "source": [
    "SOURCE: https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4RpYVFvra3M"
   },
   "outputs": [],
   "source": [
    "def saveModel(model,pathdir):\n",
    "  torch.save(model.state_dict(), pathdir)\n",
    "\n",
    "def loadModel(pathdir,modeltype):\n",
    "  match modeltype:\n",
    "    case \"basicNeuralNet\": model = basicNeuralNet(784,10).to(device)\n",
    "    case \"convNet\": model = convNet().to(device)\n",
    "\n",
    "  model.load_state_dict(torch.load(pathdir))\n",
    "  model.eval()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RN1hsUtGTHGO"
   },
   "outputs": [],
   "source": [
    "saveModel(all_models[0],\"testModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONc24AR_Wkrb"
   },
   "outputs": [],
   "source": [
    "tryModel = loadModel(\"testModel\",\"convNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCWs8hWW1MP8"
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5McLfF3cWI_8"
   },
   "source": [
    "Sparsify Interval\n",
    "Base case\n",
    "Fine-Tuning Phase (Freeze weights) , < Sparsify interval\n",
    "Real-time visualization - add trainin loss per batch and test loss, and test accuracy\n",
    "Weights and Biases\n",
    "\n",
    "\n",
    "AC/DC proof 8.1.4,\n",
    "\n",
    "Make proof on board work for large numbers, i.e.! T:(S* times Kappa^2 * some constant factor)\n",
    "Want the damage to be 1 + epsilon\n",
    "\n",
    "make sure you can collect useful information - e.g. things like sparsity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fx9FaaEd4IF"
   },
   "source": [
    "# Empirically Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E642kOUidWJT"
   },
   "outputs": [],
   "source": [
    "def testModel(model):\n",
    "  randomExampleInt = np.random.randint(1000)\n",
    "  exampleX = dataset2.data[randomExampleInt].reshape(28, 28)\n",
    "  plt.imshow(exampleX)\n",
    "  print(exampleX.shape)\n",
    "  exampleX = torch.reshape(exampleX, (1, 1,28,28))\n",
    "  predicted = model(torch.tensor(exampleX,dtype=torch.float32).to(device))\n",
    "  print(torch.argmax(predicted))\n",
    "\n",
    "testModel(tryModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnermn-gzRHe"
   },
   "source": [
    "# TO DO\n",
    "\n",
    "- check the sparsity of bias persists if we increase sparsity (95% sparsity and 99%)\n",
    "\n",
    "- compare with and without first phase I added\n",
    "\n",
    "- try to see if same spike appears with inserting SGD on decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LdUT0Hy2eal"
   },
   "source": [
    "- Visualization?\n",
    "-  Save Model? - MAYBE USEFUL\n",
    "-  Checkpoints? - YES DO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV8EnZTx2eXj"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oxXZNYnHQj9V",
    "BK4nfrXJNb14",
    "RCWs8hWW1MP8",
    "7fx9FaaEd4IF"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
