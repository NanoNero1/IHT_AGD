{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_U2DWonNx69r"
   },
   "source": [
    "### AUTHOR: Dimitri Kachler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gEAgjIot_r8h"
   },
   "source": [
    "# Global Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tny4XuKHB4VZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fTxMQlrI_rqG"
   },
   "outputs": [],
   "source": [
    "#User-Dependent Variables\n",
    "layerByLayer = False\n",
    "datasetChoice = \"MNIST\"\n",
    "\n",
    "# -------------- INACTIVE\n",
    "#useNeptune = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "okBTEBAuZX54"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "4xMEbdjEXUk3"
   },
   "outputs": [],
   "source": [
    "# Neural Networks\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.optim.optimizer import Optimizer, required\n",
    "import torch\n",
    "\n",
    "# Arrays & Mathematics\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "#Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "#System / IO\n",
    "import abc\n",
    "import itertools\n",
    "import importlib\n",
    "\n",
    "#Data Visualization\n",
    "#import seaborn as sns\n",
    "\n",
    "#External Utilities\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5eUqt83pkPVh",
    "outputId": "c825348b-ff86-4cfa-a182-ed7f773d78fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n",
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# CUDA Check\n",
    "print(torch.__version__)\n",
    "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nwmhYoV2jqci"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it worked, already imported neptune\n"
     ]
    }
   ],
   "source": [
    "# NOTE: you can still run this and it should still work and send the data to my Neptune.ai project,\n",
    "# unfortunately you won't be able to see the graph without my account\n",
    "# Capture makes it so that the cell doesn't output text\n",
    "# NOTE: %%capture doesn't work on jupyterlab\n",
    "#%%capture\n",
    "#\n",
    "try:\n",
    "    import neptune\n",
    "    print('it worked, already imported neptune')\n",
    "except ImportError as e:\n",
    "    abort()\n",
    "    %pip install -U neptune\n",
    "    import neptune\n",
    "#import neptune\n",
    "from getpass import getpass\n",
    "\n",
    "project=\"dimitri-kachler-workspace/sanity-MNIST\"\n",
    "api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNWQxNDllOS04OGY1LTRjM2EtYTczZi0xNWI0NTRmZTA1OTEifQ==\"\n",
    "#project = neptune.init_project(api_token=api_token, project=project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_bQtl6tN0vWb"
   },
   "source": [
    "# Github Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BeWatvCl4aXy",
    "outputId": "5e41612f-42fc-4e91-a870-4a400e42a726"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'IHT_AGD' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/NanoNero1/IHT_AGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LViCBrmwbiWW"
   },
   "outputs": [],
   "source": [
    "# NOTE: This might be very expenisive! - only keep active on prototype testing\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NPb-BbT41iR2",
    "outputId": "9f385b63-0227-4f08-f2bd-f405e60a64ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "#%cd /content/IHT_AGD/\n",
    "\n",
    "# NOTE: for whatever reason, this does not actually work locally, you have to manually open terminal to and do a git pull.\n",
    "\n",
    "\n",
    "# NOTE: for some reason, there is difficulty in what directory git pull should be called\n",
    "!git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Dw1TZCfWVg9p"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS IS TO CHECK THAT THE DATA IS NOT BEING DOWNLOADED EVERY TIME!\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'abort' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Data Collection\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mIHT_AGD\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loaders\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataLoaders\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdataLoaders\u001b[39;00m\n\u001b[0;32m      3\u001b[0m datasetChoice \u001b[38;5;241m=\u001b[39m dataLoaders\u001b[38;5;241m.\u001b[39mdatasetChoice\n\u001b[0;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m dataLoaders\u001b[38;5;241m.\u001b[39mtrain_loader\n",
      "File \u001b[1;32m~\\Documents\\IRIF_INTERN\\code\\testExperimentWork\\IHT_AGD\\data_loaders\\dataLoaders.py:31\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;124;03m\"\"\" Desc: The MNIST Dataset, sourced from PyTorch, this dataset is for classification of handwritten digits\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;124;03m  Size: 60,000 examples,\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124;03m  Input: 28x28 pixels,\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124;03m  Target: digit 0-9,\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     15\u001b[0m datasetChoice \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mmatch\u001b[39;00m datasetChoice:\n\u001b[0;32m     18\u001b[0m   \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMNIST\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Data Collection and Normalizing so that it's suitable for input to the neural network\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     transform\u001b[38;5;241m=\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mCompose([transforms\u001b[38;5;241m.\u001b[39mToTensor(),transforms\u001b[38;5;241m.\u001b[39mNormalize((\u001b[38;5;241m0.1307\u001b[39m,), (\u001b[38;5;241m0.3081\u001b[39m,))])\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m     \u001b[38;5;66;03m# Checking if data is already downloaded\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     p \u001b[38;5;241m=\u001b[39m Path(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data/MNIST/raw/train-images-idx3-ubyte\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTHIS IS TO CHECK THAT THE DATA IS NOT BEING DOWNLOADED EVERY TIME!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     27\u001b[0m       \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaise Error - data not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m     \n\u001b[1;32m---> 31\u001b[0m     abort()\n\u001b[0;32m     32\u001b[0m \n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m     dataset1 \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     35\u001b[0m     dataset2 \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mMNIST(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     36\u001b[0m \n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# setup information?\u001b[39;00m\n\u001b[0;32m     38\u001b[0m   \u001b[38;5;28;01mcase\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCIFAR\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;66;03m# NOTE Right now CIFAR shouldn't work - I forgot why unfortunately\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;66;03m# note to self: I just remembered! we are applying the MNIST transformation to CIFAR,\u001b[39;00m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# obviously this is a bad idea. CHECK: is CIFAR already normalized, if it isn't, normalize it\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     abort()\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m############################ OOOOOOOOOOOOOOOOH!!!!! ACTUALLY I NEED TO NORMALIZE IT< THE TRANSFORM IS WRONG!!!!\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     dataset1 \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mCIFAR10(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, download\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     45\u001b[0m     dataset2 \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mCIFAR10(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../data\u001b[39m\u001b[38;5;124m'\u001b[39m, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,transform\u001b[38;5;241m=\u001b[39mtransform)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Data Loaders : These also allow us to test performance ad-hoc\u001b[39;00m\n\u001b[0;32m     49\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset1,batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'abort' is not defined"
     ]
    }
   ],
   "source": [
    "# Data Collection\n",
    "import IHT_AGD.data_loaders.dataLoaders as dataLoaders\n",
    "datasetChoice = dataLoaders.datasetChoice\n",
    "train_loader = dataLoaders.train_loader\n",
    "test_loader = dataLoaders.test_loader\n",
    "#abort() # I should have the data loaded locally!!!!\n",
    "abort()\n",
    "# Just for debugging\n",
    "#import IHT_AGD.architectures.architect\n",
    "#IHT_AGD.architectures.architect.seeVariable\n",
    "\n",
    "# Neural Netwok Architecture\n",
    "from IHT_AGD.architectures.convNets import MNIST_convNet\n",
    "\n",
    "# Taining and Testing Functions\n",
    "from IHT_AGD.modelTrainTest.trainingMetrics import getTestAccuracy,getTestLoss\n",
    "from IHT_AGD.modelTrainTest.trainLoop import train\n",
    "\n",
    "# Optimizers (base, SGD, AGD, IHT, etc.)\n",
    "from IHT_AGD.optimizers.baseOptimizer import myOptimizer\n",
    "from IHT_AGD.optimizers.vanillaSGD import vanillaSGD\n",
    "from IHT_AGD.optimizers.ihtSGD import ihtSGD\n",
    "from IHT_AGD.optimizers.vanillaAGD import vanillaAGD\n",
    "from IHT_AGD.optimizers.ihtAGD import ihtAGD\n",
    "from IHT_AGD.optimizers.nativePytorchSGD import dimitriPytorchSGD\n",
    "\n",
    "# Visualization Functions\n",
    "from IHT_AGD.visualizationGraphs.plotting import plotMetric\n",
    "\n",
    "# Experiment Functions\n",
    "from IHT_AGD.experimentScaffolding.chooseOptimizer import chooseOptimizer\n",
    "from IHT_AGD.experimentScaffolding.chooseOptimizer import fixedChooseOptimizer\n",
    "from IHT_AGD.experimentScaffolding.experimentFuncs import runOneExperiment\n",
    "from IHT_AGD.experimentScaffolding.experimentFuncs import runMainExperiment\n",
    "from IHT_AGD.experimentScaffolding.experimentFuncs import runPipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GVWidDypEprA",
    "outputId": "e5ba04df-4347-4adc-a7aa-925de5d5e8f7"
   },
   "outputs": [],
   "source": [
    "#To know the sizes\n",
    "firstInput, firstTarget = next(iter(train_loader))\n",
    "print(firstInput.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SoV-EkH4rie"
   },
   "source": [
    "# Experiment Github Imports?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mwvZ1FqKdtjt"
   },
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9L2J1E7ifJ-7"
   },
   "outputs": [],
   "source": [
    "variablesToTrack = ['sparsity','sparsityBias','lr','iteration','trackSparsity','trackSparsityBias','trackSparsityLinear','testAccuracy','beta']\n",
    "expensiveVariables = ['testAccuracy']\n",
    "functionsToHelpTrack = ['trackingSparsity','getTestAccuracy']\n",
    "expensiveFunctions = ['getTestAccuracy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UaMmGdAu32BP"
   },
   "source": [
    "# Setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PETuCM2Z33cY",
    "outputId": "96b3ebaf-df43-4e9f-ce1a-1662dfceaf90"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# NOTE: I think it might be useful to keep the setups here, at least for now since we change the settings often\n",
    "setup_ihtAGD = {\n",
    "    \"scheme\":\"ihtAGD\" ,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":5.0,\n",
    "    \"beta\":50.0}\n",
    "\n",
    "setup_vanillaAGD = {\n",
    "    \"scheme\":\"vanillaAGD\",\n",
    "    \"kappa\":5.0,\n",
    "    \"beta\":50.0,\n",
    "    }\n",
    "\n",
    "setup_ihtSGD = {\n",
    "    \"scheme\":\"ihtSGD\" ,\n",
    "    \"sparsity\":0.900,\n",
    "    \"beta\": 50.0,}\n",
    "\n",
    "setup_vanillaSGD = {\n",
    "    \"scheme\":\"vanillaSGD\",\n",
    "    \"sparsity\":0.9,\n",
    "    \"beta\": 50.0,\n",
    "}\n",
    "\n",
    "setup_pytorchSGD = {\n",
    "    \"scheme\":\"pytorchSGD\"\n",
    "}\n",
    "\n",
    "setup_ihtAGD = {\n",
    "    \"scheme\":\"ihtAGD\" ,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":5.0,\n",
    "    \"beta\":50.0}\n",
    "\"\"\"\n",
    "print('new setups')\n",
    "\n",
    "experimentName = 'differentSparsities'\n",
    "setups = None\n",
    "exec(f\"import IHT_AGD.setups.setup_{experimentName}\")\n",
    "exec(f\"setups = IHT_AGD.setups.setup_{experimentName}.setups\")\n",
    "print(setups)\n",
    "#abort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q25ZdCrjzVZd"
   },
   "source": [
    "# Running the Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gX9f4erHCDCE",
    "outputId": "b22b3f6b-a0fb-4f96-e614-2fdddaf0a09d"
   },
   "outputs": [],
   "source": [
    "print(datasetChoice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "TTG9HAmZRlwo",
    "outputId": "b62107f7-0e69-45db-c5ef-a7724758a9fd"
   },
   "outputs": [],
   "source": [
    "\"\"\" MAIN CELL \"\"\"\n",
    "#setups = [setup_ihtAGD]#,setup_vanillaSGD]#,setup_ihtAGD]\n",
    "#setups = [setup_pytorchSGD]\n",
    "print(setups)\n",
    "\n",
    "\n",
    "run = neptune.init_run(api_token=api_token, project=project)\n",
    "runPipeline(setups,\n",
    "            datasetChoice=\"MNIST\",\n",
    "            epochs=10,trials=5,\n",
    "            functionsToHelpTrack=functionsToHelpTrack,\n",
    "            variablesToTrack=variablesToTrack,\n",
    "            expensiveVariables=expensiveVariables,\n",
    "            expensiveFunctions=expensiveFunctions,\n",
    "            \n",
    "            \n",
    "            device=device,\n",
    "            run=run,\n",
    "            test_loader=test_loader,\n",
    "            train_loader=train_loader)\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0zfWqas3i7d0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HFXXFE2tYg-c"
   },
   "outputs": [],
   "source": [
    "%reload(runPipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3M0HjGxI2uK9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2bSdr-W0lFkq"
   },
   "outputs": [],
   "source": [
    "importlib.reload(IHT_AGD.experimentScaffolding.experimentFuncs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPnrL5azlFfC"
   },
   "outputs": [],
   "source": [
    "abort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s-LBtmTsUiSM"
   },
   "source": [
    "# -----------------------------------------------------------------------\n",
    "# END OF THE BASELINE FRAMEWORK, NEXT SECTION DEDICATED TO EXTENSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prSnPcFPUna-"
   },
   "source": [
    "## Bias Left Untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bcAf_EsRUq0w"
   },
   "outputs": [],
   "source": [
    "class untouchedIhtAGD(ihtAGD):\n",
    "  def __init__(self,params,sparsity=0.9,kappa=5.0,beta=50.0):\n",
    "    super().__init__(params)\n",
    "    self.methodName = \"untouched_iht_AGD\"\n",
    "    self.alpha = beta / kappa\n",
    "    self.beta = beta\n",
    "    self.kappa = kappa\n",
    "\n",
    "  def sparsify(self):\n",
    "    # TO-DO: remember to remove this zero, it is inconsequential, but still remove it in good practice\n",
    "    concatWeights = torch.zeros((1)).to(device)\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "\n",
    "        #Skip Bias Layers\n",
    "        if len(p.data.shape) < 2:\n",
    "          continue\n",
    "\n",
    "        flatWeights = torch.flatten(p.data)\n",
    "        concatWeights = torch.cat((concatWeights,flatWeights),0)\n",
    "\n",
    "    topK = int(len(concatWeights)*(1-self.sparsity))\n",
    "    vals, bestI = torch.topk(torch.abs(concatWeights),topK,dim=0)\n",
    "    cutoff = vals[-1]\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "\n",
    "        #Skip Bias Layers\n",
    "        if len(p.data.shape) < 2:\n",
    "          continue\n",
    "\n",
    "        p.data[abs(p.data) <= cutoff] = 0.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lS7mdfZIXedV"
   },
   "outputs": [],
   "source": [
    "setup_untouched_ihtAGD = {\n",
    "    \"scheme\":\"untouchedIhtAGD\",\n",
    "    \"lr\":0.1,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":10.0,\n",
    "    \"beta\":100.0}\n",
    "setups = [setup_untouched_ihtAGD, setup_ihtAGD]\n",
    "\n",
    "run = neptune.init_run(api_token=api_token, project=project)\n",
    "all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups)\n",
    "run.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oxXZNYnHQj9V"
   },
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_i4nU5QFQl3j"
   },
   "outputs": [],
   "source": [
    "from os import setgroups\n",
    "\n",
    "def gridSearch(default,variables,values,metric,epochs=1):\n",
    "  \"\"\" Desc: searches in a grid for the best combination of values of arbitrary dimension,\n",
    "        we can check for more than 2 variables at a time, but this can be very costly\n",
    "\n",
    "  default [dictionary]: a dictionary for all the default settings, this is also how one can set the type of algorithm\n",
    "  variables [array[string]]: the settings to change\n",
    "  values [2Darray]: what values to take on\n",
    "  metric [string]: what metric to use for the best value\n",
    "  \"\"\"\n",
    "\n",
    "  # We will not know how to traverse this list easily however\n",
    "  # TO-DO: find a way to organize, or traverse this list\n",
    "  setups = []\n",
    "\n",
    "  # This list has every possible combination of the settings\n",
    "  valuePermutations = list(itertools.product(*values))\n",
    "\n",
    "  for permutation in valuePermutations:\n",
    "    newSetup = default.copy()\n",
    "    for idx,val in enumerate(permutation):\n",
    "\n",
    "      # Adjusts the settings one-by-one\n",
    "      newSetup[variables[idx]] = val\n",
    "\n",
    "    setups.append(newSetup)\n",
    "\n",
    "  print(setups)\n",
    "\n",
    "\n",
    "  all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups,epochs=epochs)\n",
    "\n",
    "  # NEXT: Interchange with a different metric\n",
    "  # TO-DO: try \"highest loss\" over entire dataset using model\n",
    "\n",
    "  # Right now we use the accuracy in after the last epoch\n",
    "  # BUG: is the last epoch at 0 or -1 I need to check\n",
    "  min_accuracies = [accuracies[-1] for accuracies in all_accuracies]\n",
    "  bestSetupIndex = min_accuracies.index(min(min_accuracies))\n",
    "\n",
    "\n",
    "\n",
    "  return setups[bestSetupIndex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdkn9Z7tHt7y"
   },
   "outputs": [],
   "source": [
    "default = {\n",
    "    \"scheme\":\"vanillaAGD\",\n",
    "    \"lr\":0.1,\n",
    "    \"sparsity\":0.90,\n",
    "    \"kappa\":15.0,\n",
    "    \"beta\":10000.0}\n",
    "# We set a big value to see if we overwrite it in the Grid Search\n",
    "\n",
    "gridSearch(default,[\"kappa\",\"beta\"],[[2.0,10.0,100.0],[10.0,100.0,300.0]],\"loss\",5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NCz_DpeoYqoK"
   },
   "outputs": [],
   "source": [
    "#This works! It recognizes it as a class name\n",
    "type(eval(\"ihtAGD\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mTrAB1jIWvH"
   },
   "source": [
    "# **Appendix**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BK4nfrXJNb14"
   },
   "source": [
    "# Saving and Loading Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EVY_NTm3OKeX"
   },
   "source": [
    "SOURCE: https://pytorch.org/tutorials/beginner/saving_loading_models.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4RpYVFvra3M"
   },
   "outputs": [],
   "source": [
    "def saveModel(model,pathdir):\n",
    "  torch.save(model.state_dict(), pathdir)\n",
    "\n",
    "def loadModel(pathdir,modeltype):\n",
    "  match modeltype:\n",
    "    case \"basicNeuralNet\": model = basicNeuralNet(784,10).to(device)\n",
    "    case \"convNet\": model = convNet().to(device)\n",
    "\n",
    "  model.load_state_dict(torch.load(pathdir))\n",
    "  model.eval()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RN1hsUtGTHGO"
   },
   "outputs": [],
   "source": [
    "saveModel(all_models[0],\"testModel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ONc24AR_Wkrb"
   },
   "outputs": [],
   "source": [
    "tryModel = loadModel(\"testModel\",\"convNet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RCWs8hWW1MP8"
   },
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5McLfF3cWI_8"
   },
   "source": [
    "Sparsify Interval\n",
    "Base case\n",
    "Fine-Tuning Phase (Freeze weights) , < Sparsify interval\n",
    "Real-time visualization - add trainin loss per batch and test loss, and test accuracy\n",
    "Weights and Biases\n",
    "\n",
    "\n",
    "AC/DC proof 8.1.4,\n",
    "\n",
    "Make proof on board work for large numbers, i.e.! T:(S* times Kappa^2 * some constant factor)\n",
    "Want the damage to be 1 + epsilon\n",
    "\n",
    "make sure you can collect useful information - e.g. things like sparsity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fx9FaaEd4IF"
   },
   "source": [
    "# Empirically Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E642kOUidWJT"
   },
   "outputs": [],
   "source": [
    "def testModel(model):\n",
    "  randomExampleInt = np.random.randint(1000)\n",
    "  exampleX = dataset2.data[randomExampleInt].reshape(28, 28)\n",
    "  plt.imshow(exampleX)\n",
    "  print(exampleX.shape)\n",
    "  exampleX = torch.reshape(exampleX, (1, 1,28,28))\n",
    "  predicted = model(torch.tensor(exampleX,dtype=torch.float32).to(device))\n",
    "  print(torch.argmax(predicted))\n",
    "\n",
    "testModel(tryModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pnermn-gzRHe"
   },
   "source": [
    "# TO DO\n",
    "\n",
    "- check the sparsity of bias persists if we increase sparsity (95% sparsity and 99%)\n",
    "\n",
    "- compare with and without first phase I added\n",
    "\n",
    "- try to see if same spike appears with inserting SGD on decompression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1LdUT0Hy2eal"
   },
   "source": [
    "- Visualization?\n",
    "-  Save Model? - MAYBE USEFUL\n",
    "-  Checkpoints? - YES DO THIS!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tV8EnZTx2eXj"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "oxXZNYnHQj9V",
    "BK4nfrXJNb14",
    "RCWs8hWW1MP8",
    "7fx9FaaEd4IF"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
