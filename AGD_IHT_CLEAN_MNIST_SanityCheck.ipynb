{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NanoNero1/IHT_AGD/blob/main/AGD_IHT_CLEAN_MNIST_SanityCheck.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AUTHOR: Dimitri Kachler"
      ],
      "metadata": {
        "id": "_U2DWonNx69r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This is a test cell for github"
      ],
      "metadata": {
        "id": "HHCCAhllyW4s"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEAgjIot_r8h"
      },
      "source": [
        "# Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTxMQlrI_rqG"
      },
      "outputs": [],
      "source": [
        "#User-Dependent Variables\n",
        "layerByLayer = False\n",
        "\n",
        "# -------------- INACTIVE\n",
        "#useNeptune = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okBTEBAuZX54"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4xMEbdjEXUk3"
      },
      "outputs": [],
      "source": [
        "# Neural Networks\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "\n",
        "# Arrays & Mathematics\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#System / IO\n",
        "import abc\n",
        "import itertools\n",
        "\n",
        "#External Utilities\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eUqt83pkPVh",
        "outputId": "c44ebb6d-1ca7-4ecf-9103-23c8e6152179"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.3.0+cu121\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "# CUDA Check\n",
        "print(torch.__version__)\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7xy8XUrWZVLh"
      },
      "source": [
        "# Dataset Collection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KQFJQFZBX5SF"
      },
      "source": [
        "SOURCE: https://colab.research.google.com/github/skorch-dev/skorch/blob/master/notebooks/MNIST.ipynb#scrollTo=mwpfASvc_v7J"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HArnrnDIViL5"
      },
      "source": [
        "SOURCE: https://github.com/pytorch/examples/blob/main/mnist/main.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dw1TZCfWVg9p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbd13e6a-ea0c-4e56-9aeb-16e2013bcf0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 54503292.75it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 12113569.38it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "100%|██████████| 1648877/1648877 [00:00<00:00, 82858785.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5070675.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "\"\"\" Desc: The MNIST Dataset, sourced from PyTorch, this dataset is for classification of handwritten digits\n",
        "  Size: 60,000 examples,\n",
        "  Input: 28x28 pixels,\n",
        "  Target: digit 0-9,\n",
        "\"\"\"\n",
        "\n",
        "# Data Collection and Normalizing so that it's suitable for input to the neural network\n",
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "dataset1 = datasets.MNIST('../data', train=True, download=True,\n",
        "                       transform=transform)\n",
        "dataset2 = datasets.MNIST('../data', train=False,\n",
        "                       transform=transform)\n",
        "\n",
        "\n",
        "# Data Loaders : These also allow us to test performance ad-hoc\n",
        "train_loader = torch.utils.data.DataLoader(dataset1,batch_size=1000,shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2,batch_size=1000,shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsJC9k_wdNVe",
        "outputId": "cd933eec-d3f3-43f3-eb29-689563c596af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torchvision.datasets.mnist.MNIST'>\n"
          ]
        }
      ],
      "source": [
        "print(type(dataset1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o84gWHX2Y8mO"
      },
      "source": [
        "# Building the Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaynrLodnoYZ"
      },
      "source": [
        "SOURCE: https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html\n",
        "\n",
        "SOURCE: https://colab.research.google.com/github/omarsar/pytorch_notebooks/blob/\n",
        "[master/pytorch_quick_start.ipynb#scrollTo=3_0Vjq2RHlph](https://colab.research.google.com/github/omarsar/pytorch_notebooks/blob/master/pytorch_quick_start.ipynb#scrollTo=3_0Vjq2RHlph)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHcSg3GBZFEt"
      },
      "outputs": [],
      "source": [
        "### DEPRECATED\n",
        "class basicNeuralNet(nn.Module):\n",
        "  def __init__(self,inputDim,outputDim):\n",
        "    super(basicNeuralNet, self).__init__()\n",
        "    # Hyper-Parameters\n",
        "    hiddenSize_1 = 200\n",
        "    hiddenSize_2 = 50\n",
        "\n",
        "    # Pytorch Layers\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.hidden1 = nn.Linear(inputDim,hiddenSize_1)\n",
        "    self.hidden2 = nn.Linear(hiddenSize_1,hiddenSize_2)\n",
        "    self.prob = nn.Linear(hiddenSize_2,outputDim)\n",
        "\n",
        "    # Activation Functions\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "  # Forward Pass: outputs a vector [x] of softmax probabilities\n",
        "  def forward(self,x):\n",
        "    flat = self.flatten(x)\n",
        "    hidden_1 = self.relu(self.hidden1(flat))\n",
        "    hidden_2 = self.relu(self.hidden2(hidden_1))\n",
        "    prob = self.prob(hidden_2)\n",
        "    out =  F.log_softmax(prob, dim=1)\n",
        "    return out\n",
        "\n",
        "\"\"\" Desc: A basic Convolutional Neural Network : these are particularly suitable for image-based problems\n",
        "\"\"\"\n",
        "class convNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(convNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 5, 1)\n",
        "        self.fc1 = nn.Linear(4608, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    # Forward Pass: outputs a vector [x] of softmax probabilities\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        # Softmax so that we output probabilities (i.e. adds up to 1)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xb74fDQXaC4A"
      },
      "source": [
        "# Training & Testing Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weDfp31_nzpm"
      },
      "source": [
        "SOURCE: https://github.com/pytorch/examples/tree/main/mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKL5amcRZWf5"
      },
      "outputs": [],
      "source": [
        "\"\"\" Desc: function for one training epoch. At each step, we query the optimizer and log the training loss to Neptune.\n",
        "  Output: training loss, testing loss, [! we do not capture testing accuracy here]\n",
        "\"\"\"\n",
        "\n",
        "def train(args, model, device, train_loader, optimizer, epoch,trialNumber=None):\n",
        "    model.train()\n",
        "\n",
        "    train_epoch_losses = []\n",
        "    test_epoch_losses = []\n",
        "\n",
        "    # In case we get NaN, setting this to true should detect this\n",
        "    # BUG: for some reason this fails to capture NaNs\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    # Making sure we know what scheme is implemented\n",
        "    print(optimizer.methodName)\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Generating Predictions and Calculating Loss\n",
        "        output = model(data)\n",
        "        loss = F.nll_loss(output, target)\n",
        "\n",
        "        # Logs to Neptune AI\n",
        "        run[f\"trials/{optimizer.trialNumber}/training/batch/loss/{optimizer.methodName}/\"].append(loss)\n",
        "\n",
        "        ### DEPRECATED\n",
        "        # Saving Loss Data\n",
        "        train_epoch_losses.append(loss.cpu().detach().numpy())\n",
        "        if batch_idx % 5 == 0:\n",
        "          #test_epoch_losses.append(getTestLoss(model, device, test_loader))\n",
        "\n",
        "          # For now we are not collecting the test loss -- we will do this later on\n",
        "          test_epoch_losses.append(0)\n",
        "\n",
        "        # Backwards Propogation and Calling the Optimizer\n",
        "        loss.backward()\n",
        "\n",
        "        #NOTE: Is there going to be a naming conflict with 'params'?\n",
        "        def getNewGrad(params):\n",
        "          model.params = params\n",
        "\n",
        "          newOutput = model(data)\n",
        "          loss = F.nll_loss(newOutput, target)\n",
        "          loss.backward()\n",
        "\n",
        "        # New Optimization Step\n",
        "        optimizer.step(getNewGrad=getNewGrad)\n",
        "        #print(f\"learning rate: {optimizer.lr}\")\n",
        "\n",
        "        ### DEPRECATED - On every 10 iterations, we can print out some information\n",
        "        if batch_idx % 10 == 0 :\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    return train_epoch_losses,test_epoch_losses\n",
        "\n",
        "\n",
        "def getTestLoss(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "      data, target = next(iter(test_loader))\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "\n",
        "      # Negative Log-Likelihood Loss\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "    # The total loss is divided by the batch size to get the average\n",
        "    test_loss /= test_loader.batch_size\n",
        "    return test_loss\n",
        "\n",
        "\n",
        "def getTestAccuracy(model, device, test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    # The testing accuracy is taken over the entire dataset\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            # If the index of the maximum probability matches the target digit, we add it to the corrrect counter variable\n",
        "            pred = output.argmax(dim=1, keepdim=True)\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    accuracy = correct / len(test_loader.dataset)\n",
        "\n",
        "    print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        correct, len(test_loader.dataset),\n",
        "        100. * accuracy))\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAvL2RrcXUuK"
      },
      "source": [
        "# Custom Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VyVs2uCCaeli"
      },
      "source": [
        "SOURCE: https://www.geeksforgeeks.org/custom-optimizers-in-pytorch/\n",
        "\n",
        "SOURCE: https://github.com/azotlichid/adaptive-opt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KB7hFSKtrzN"
      },
      "source": [
        "# Test Optimizer Module?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihhTM6NFtrcM"
      },
      "outputs": [],
      "source": [
        "\"\"\" Desc: The base optimizer class inherits from the PyTorch Optimizer class. The idea of this design is that we can make brand new\n",
        "optimizers without having to re-write a lot of the boilerplate code. It also makes it really easy to combine different methods,\n",
        "e.g. Iterative Hard Thresholding with Accelerated Gradient Descent is a combination of IHT-SGD and vanilla-AGD.\n",
        "\"\"\"\n",
        "\n",
        "class myOptimizer(Optimizer):\n",
        "  def __init__(self,params,lr=1.0,sparse=False,demoValue=100):\n",
        "\n",
        "    # A dummy value to see if inheritance works properly\n",
        "    self.demoValue = demoValue\n",
        "\n",
        "    self.lr = lr\n",
        "    defaults = dict(lr=lr,sparse=sparse)\n",
        "    super(myOptimizer, self).__init__(params,defaults) # For pytorch's Optimizer class\n",
        "\n",
        "    self.iteration = 0\n",
        "    self.trialNumber = None\n",
        "\n",
        "    self.methodName=\"base_optimizer\"\n",
        "\n",
        "    ### These methods should be overridden after inheritance\n",
        "    \"\"\" Desc: the main function that the optimizer gets called on every iteration \"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def step(self,getNewGrad):\n",
        "      return None\n",
        "\n",
        "    \"\"\" Desc: what optimization scheme it uses to update weights, e.g. Accelerated Gradient Descent \"\"\"\n",
        "    @abc.abstractmethod\n",
        "    def updateWeights(self):\n",
        "      pass\n",
        "\n",
        "    ### Methods that every optimizer needs to use\n",
        "\n",
        "    def toNeptuneLoss():\n",
        "      pass\n",
        "\n",
        "    def logging():\n",
        "      pass\n",
        "\n",
        "# ----------------------------------------------------Vanilla SGD\n",
        "\n",
        "class vanillaSGD(myOptimizer):\n",
        "  def __init__(self,params,lr=1.0):\n",
        "    super(vanillaSGD, self).__init__(params, lr=lr)\n",
        "    self.methodName = \"vanilla_SGD\"\n",
        "\n",
        "  def step(self,getNewGrad):\n",
        "    self.updateWeights()\n",
        "    self.iteration +=1\n",
        "    return None\n",
        "\n",
        "  # Regular Gradient Descent\n",
        "  def updateWeights(self):\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        #Everyone gets updated at the same time because it is easier\n",
        "        p.data -= p.grad*self.lr\n",
        "\n",
        "# ---------------------------------------------------- IHT SGD\n",
        "\n",
        "class ihtSGD(vanillaSGD):\n",
        "  def __init__(self, params, lr=1.0,getNewGrad=None,sparsity=0.9,sparsifyInterval=10):\n",
        "    super(ihtSGD, self).__init__(params, lr=lr)\n",
        "    self.sparsity = sparsity\n",
        "    self.sparsifyInterval = sparsifyInterval\n",
        "\n",
        "    # For freezing weights\n",
        "    self.frozen = [torch.ones_like((p.to(device))) for p in self.param_groups[0]['params']]\n",
        "\n",
        "    self.methodName = \"iht_SGD\"\n",
        "\n",
        "  def step(self,getNewGrad):\n",
        "    self.updateWeights()\n",
        "    self.sparsify()\n",
        "    self.iteration +=1\n",
        "    return None\n",
        "\n",
        "  \"\"\" Desc: sparsifies the current weights according to a truncation operator - implementations may vary \"\"\"\n",
        "  def sparsify(self):\n",
        "    concatWeights = torch.zeros((1)).to(device)\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        flatWeights = torch.flatten(p.data)\n",
        "        concatWeights = torch.cat((concatWeights,flatWeights),0)\n",
        "\n",
        "    # Converting the sparsity factor into an integer of respective size\n",
        "    topK = int(len(concatWeights)*(1-self.sparsity))\n",
        "\n",
        "    # All the top-k values are sorted in order, we take the last one as the cutoff\n",
        "    vals, bestI = torch.topk(torch.abs(concatWeights),topK,dim=0)\n",
        "    cutoff = vals[-1]\n",
        "\n",
        "    # Zero-ing out values below the cutoff\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "        p.data[abs(p.data) <= cutoff] = 0.0\n",
        "\n",
        "  \"\"\" Desc: freezes the previously truncated weights - should be done before sparsification and during the freezing phase\"\"\"\n",
        "  ### INACTIVE, but IMPLEMENTED\n",
        "  def freeze(self):\n",
        "    for layerIdx,layer in enumerate(self.frozen):\n",
        "      self.zt[layerIdx] = self.frozen[layerIdx] * self.zt[layerIdx]\n",
        "      self.xt[layerIdx] = self.frozen[layerIdx] * self.xt[layerIdx]\n",
        "      #self.param_groups[0]['params'][layerIdx] = self.frozen[layerIdx] * self.param_groups[0]['params'][layerIdx]\n",
        "      self.param_groups[0]['params'][layerIdx].data *= self.frozen[layerIdx]\n",
        "    else:\n",
        "      print('regular step')\n",
        "\n",
        "  # TODO: move the code from the old AGD class here\n",
        "  #def updateFrozen():\n",
        "\n",
        "\n",
        "# ---------------------------------------------------- Vanilla AGD\n",
        "\n",
        "class vanillaAGD(vanillaSGD):\n",
        "  def __init__(self,params,kappa=5.0,beta=50.0):\n",
        "    super(vanillaAGD, self).__init__(params)\n",
        "    self.methodName = \"vanilla_AGD\"\n",
        "\n",
        "    # Objective Function Property Variables\n",
        "    # NOTE: good values are found with Grid Search\n",
        "    self.alpha = beta / kappa\n",
        "    self.beta = beta\n",
        "    self.kappa = kappa\n",
        "    self.sqKappa = pow(self.kappa,0.5)\n",
        "\n",
        "    # Initializing the internal storage for the iterates\n",
        "    self.zt = [torch.zeros_like((p.to(device))) for p in self.param_groups[0]['params']]\n",
        "    self.xt = [p for p in self.param_groups[0]['params']]\n",
        "\n",
        "    # Compression, Decompression and Freezing Variables\n",
        "    self.phaseLength = 100\n",
        "    self.compressionRatio = 0.5\n",
        "    self.freezingRatio = 0.5\n",
        "\n",
        "    self.methodName = \"vanilla_AGD\"\n",
        "\n",
        "  def step(self,getNewGrad):\n",
        "    self.updateWeights(getNewGrad)\n",
        "    self.iteration += 1\n",
        "\n",
        "\n",
        "  \"\"\" Desc: a function that decides wether to process in the compressed or decompressed phase, or the freezing phase\"\"\"\n",
        "  # INACTIVE, NOT TESTED\n",
        "  def compressOrDecompress(self,getNewGrad):\n",
        "    howFarAlong = self.iteration % self.phaseLength\n",
        "\n",
        "    if howFarAlong >= self.phaseLength * self.compressionRatio:\n",
        "      self.updateWeights(getNewGrad)\n",
        "    elif howFarAlong >= self.phaseLength * self.compressionRatio * self.freezingRatio:\n",
        "      # INACTIVE\n",
        "      self.updateWeights(getNewGrad)\n",
        "      self.sparsify()\n",
        "      # NOTE: is the order correct?\n",
        "      self.freeze()\n",
        "    else:\n",
        "      self.updateWeights(getNewGrad)\n",
        "      self.sparsify()\n",
        "\n",
        "  def updateWeights(self,getNewGrad):\n",
        "\n",
        "    ############################## DO NOT FORGET ###################################\n",
        "    # For efficiency purposes I will need to store old gradient z_{t-1} because we re-use it in the current iteration\n",
        "    # right now it takes 1.5 as much computation as we really need\n",
        "    ################################################################################\n",
        "\n",
        "    # We personally query a new gradient from a custom iterate z_{t-1}\n",
        "    getNewGrad(self.zt)\n",
        "\n",
        "    # Update z_t the according to the AGD equation in the note\n",
        "    for layerIdx,layer in enumerate(self.zt):\n",
        "      self.zt[layerIdx] = (self.sqKappa / (self.sqKappa + 1.0) ) * (self.zt[layerIdx] - self.param_groups[0]['params'][layerIdx].grad / self.beta) + (1.0 / (self.sqKappa + 1.0)) * self.xt[layerIdx]\n",
        "\n",
        "    # This is actually \\grad z_t\n",
        "    getNewGrad(self.zt)\n",
        "\n",
        "    # Final Step of Accelerated Gradient Descent\n",
        "    for group in self.param_groups:\n",
        "      for pInd,p in enumerate(group['params']):\n",
        "        #Everyone gets updated at once because it is easier to implement\n",
        "        p.data -= p.grad*(1.0 / pow(self.alpha*self.beta,0.5))\n",
        "\n",
        "# ---------------------------------------------------- IHT AGD\n",
        "\n",
        "class ihtAGD(vanillaAGD,ihtSGD):\n",
        "  def __init__(self,params,sparsity=0.9,kappa=5.0,beta=50.0):\n",
        "    super(ihtAGD, self).__init__(params)\n",
        "    self.methodName = \"iht_AGD\"\n",
        "    self.alpha = beta / kappa\n",
        "    self.beta = beta\n",
        "    self.kappa = kappa\n",
        "\n",
        "    # BUG: Because of multiple inheritance I need to add this statement below,\n",
        "    # but I know how to fix this\n",
        "    self.sparsity=sparsity\n",
        "\n",
        "  def step(self,getNewGrad):\n",
        "    self.updateWeights(getNewGrad)\n",
        "    self.sparsify()\n",
        "    self.iteration += 1\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzvjq53Ky60k"
      },
      "source": [
        "# Experiment Trial Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEkG4xfky6eD"
      },
      "outputs": [],
      "source": [
        "# NOTE: we want to remove this and replace it with a better system,\n",
        "# nonetheless, this is still useful for now due to a small issue in my setup code\n",
        "def chooseOptimizer(setup,model,trialNumber):\n",
        "  match setup[\"scheme\"]:\n",
        "    case \"vanillaSGD\":\n",
        "         optimizer = vanillaSGD(model.parameters(),lr=setup[\"lr\"])\n",
        "    case \"ihtSGD\":\n",
        "         optimizer = ihtSGD(model.parameters(),sparsity=setup[\"sparsity\"],lr=setup[\"lr\"])\n",
        "    case \"vanillaAGD\":\n",
        "         optimizer = vanillaAGD(model.parameters(),kappa=setup[\"kappa\"],beta=setup[\"beta\"])\n",
        "    case \"ihtAGD\":\n",
        "         optimizer = ihtAGD(model.parameters(),sparsity=setup[\"sparsity\"],kappa=setup[\"kappa\"],beta=setup[\"beta\"])\n",
        "    case \"untouchedIhtAGD\":\n",
        "        optimizer = untouchedIhtAGD(model.parameters(),sparsity=setup[\"sparsity\"],kappa=setup[\"kappa\"],beta=setup[\"beta\"])\n",
        "    case _:\n",
        "        pass\n",
        "        #action-default\n",
        "  optimizer.trialNumber = trialNumber\n",
        "  return optimizer\n",
        "\n",
        "def runOneExperiment(setup=None,trialNumber=None):\n",
        "  model = convNet().to(device)\n",
        "\n",
        "  optimizer = chooseOptimizer(setup,model,trialNumber)\n",
        "  #optimizer = eval(setup[\"scheme\"])(setup,model,trialNumber)\n",
        "\n",
        "  training_losses = []\n",
        "  testing_losses = []\n",
        "  testing_accuracies = []\n",
        "\n",
        "  # This implementation uses a Learning Rate Scheduler\n",
        "  # TO-DO: test if it actually changes the learning rate, I suspect it isn't working\n",
        "  scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1000, gamma=0.1)\n",
        "  for epoch in range(1, setup[\"epochs\"] + 1):\n",
        "\n",
        "    print(optimizer.methodName)\n",
        "\n",
        "    # Call to run one epoch of training\n",
        "    train_epoch_loss,test_epoch_loss = train([],model, device, train_loader, optimizer, epoch,trialNumber)\n",
        "\n",
        "    training_losses.extend(train_epoch_loss)\n",
        "    testing_losses.extend(test_epoch_loss)\n",
        "\n",
        "\n",
        "    testAccuracy = getTestAccuracy(model, device, test_loader)\n",
        "\n",
        "    # Logs to Neptune AI\n",
        "    run[f\"trials/{optimizer.trialNumber}/training/batch/accuracy/{optimizer.methodName}/\"].append(testAccuracy)\n",
        "\n",
        "    testing_accuracies.append(testAccuracy)\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "  print(testing_accuracies)\n",
        "  return model,training_losses,testing_losses,testing_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wPr_DT9jXAOl"
      },
      "outputs": [],
      "source": [
        "def runMainExperiment(setups,epochs=5):\n",
        "  defaults = {\"epochs\":epochs}\n",
        "\n",
        "  # Combines the default setup with some added parameters\n",
        "  setups = [defaults | setup for setup in setups]\n",
        "\n",
        "  print(setups)\n",
        "\n",
        "  all_training_losses = [[] for i in range((len(setups)))]\n",
        "  all_testing_losses = [[] for i in range((len(setups)))]\n",
        "  all_models = [[] for i in range((len(setups)))]\n",
        "  all_accuracies = [[] for i in range((len(setups)))]\n",
        "  for idx in range(len(setups)):\n",
        "    all_models[idx],all_training_losses[idx],all_testing_losses[idx],all_accuracies[idx] = runOneExperiment(setups[idx],idx)\n",
        "\n",
        "  return all_models,all_training_losses,all_testing_losses,all_accuracies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaMmGdAu32BP"
      },
      "source": [
        "# Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PETuCM2Z33cY"
      },
      "outputs": [],
      "source": [
        "setup_ihtAGD = {\n",
        "    \"scheme\":\"ihtAGD\" ,\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.90,\n",
        "    \"kappa\":3.0,\n",
        "    \"beta\":50.0}\n",
        "\n",
        "setup_vanillaAGD = {\n",
        "    \"scheme\":\"vanillaAGD\",\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.9,\n",
        "    \"kappa\":3.0,\n",
        "    \"beta\":50.0,}\n",
        "# NOTE: you can add in more variables than necessary, e.g. AGD doesn't use the learning rate 'lr'\n",
        "# NOTE 2: do we want it to ignore extra parameters? so far ignoring has been useful in the implementation\n",
        "\n",
        "setup_ihtSGD = {\n",
        "    \"scheme\":\"ihtSGD\" ,\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.90,}\n",
        "\n",
        "setup_vanillaSGD = {\n",
        "    \"scheme\":\"vanillaSGD\",\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.9,}\n",
        "\n",
        "#IMPORTANT! re-run this in the same cell, otherwise if you change the setups it won't work for some reason (maybe because the array casts the variables)\n",
        "setups = [setup_ihtAGD,setup_vanillaAGD,setup_ihtSGD,setup_vanillaSGD]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjwFP3khZHNJ"
      },
      "source": [
        "# Neptune.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1LSpUXsZEdB",
        "outputId": "bde8eb88-d48a-4cc0-b6bd-6ac8e3c21492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting neptune\n",
            "  Downloading neptune-1.10.4-py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.6/502.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython>=2.0.8 (from neptune)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Pillow>=1.1.6 in /usr/local/lib/python3.10/dist-packages (from neptune) (9.4.0)\n",
            "Requirement already satisfied: PyJWT in /usr/lib/python3/dist-packages (from neptune) (2.3.0)\n",
            "Collecting boto3>=1.28.0 (from neptune)\n",
            "  Downloading boto3-1.34.111-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado<12.0.0,>=11.0.0 (from neptune)\n",
            "  Downloading bravado-11.0.3-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (8.1.7)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from neptune) (0.18.3)\n",
            "Requirement already satisfied: oauthlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (3.2.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from neptune) (24.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from neptune) (2.0.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from neptune) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.31.0)\n",
            "Requirement already satisfied: requests-oauthlib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.3.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.16.0)\n",
            "Collecting swagger-spec-validator>=2.7.4 (from neptune)\n",
            "  Downloading swagger_spec_validator-3.0.3-py2.py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (4.11.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from neptune) (2.0.7)\n",
            "Requirement already satisfied: websocket-client!=1.0.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from neptune) (1.8.0)\n",
            "Collecting botocore<1.35.0,>=1.34.111 (from boto3>=1.28.0->neptune)\n",
            "  Downloading botocore-1.34.111-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.28.0->neptune)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.28.0->neptune)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bravado-core>=5.16.1 (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading bravado-core-6.1.1.tar.gz (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (1.0.8)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (2.8.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from bravado<12.0.0,>=11.0.0->neptune) (6.0.1)\n",
            "Collecting simplejson (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading simplejson-3.19.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.9/137.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting monotonic (from bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython>=2.0.8->neptune)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20.0->neptune) (2024.2.2)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from swagger-spec-validator>=2.7.4->neptune) (4.19.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (2024.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas->neptune) (1.25.2)\n",
            "Collecting jsonref (from bravado-core>=5.16.1->bravado<12.0.0,>=11.0.0->neptune)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython>=2.0.8->neptune)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (0.18.1)\n",
            "Collecting fqdn (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading fqdn-1.5.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting isoduration (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading isoduration-20.11.0-py3-none-any.whl (11 kB)\n",
            "Collecting jsonpointer>1.13 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Collecting rfc3339-validator (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3339_validator-0.1.4-py2.py3-none-any.whl (3.5 kB)\n",
            "Collecting rfc3986-validator>0.1.0 (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading rfc3986_validator-0.1.1-py2.py3-none-any.whl (4.2 kB)\n",
            "Collecting uri-template (from jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading uri_template-1.3.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: webcolors>=1.11 in /usr/local/lib/python3.10/dist-packages (from jsonschema->swagger-spec-validator>=2.7.4->neptune) (1.13)\n",
            "Collecting arrow>=0.15.0 (from isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading arrow-1.3.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.4/66.4 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting types-python-dateutil>=2.8.10 (from arrow>=0.15.0->isoduration->jsonschema->swagger-spec-validator>=2.7.4->neptune)\n",
            "  Downloading types_python_dateutil-2.9.0.20240316-py3-none-any.whl (9.7 kB)\n",
            "Building wheels for collected packages: bravado-core\n",
            "  Building wheel for bravado-core (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bravado-core: filename=bravado_core-6.1.1-py2.py3-none-any.whl size=67672 sha256=5a49c8cb3a311f4af6f3a9ed6cda7c39c1348cb0850d08c7ae254388cc9b494e\n",
            "  Stored in directory: /root/.cache/pip/wheels/42/35/4a/44ec4c358db21a5d63ed4e40f0f0012a438106f220bce4ccba\n",
            "Successfully built bravado-core\n",
            "Installing collected packages: monotonic, uri-template, types-python-dateutil, smmap, simplejson, rfc3986-validator, rfc3339-validator, jsonref, jsonpointer, jmespath, fqdn, gitdb, botocore, arrow, s3transfer, isoduration, GitPython, swagger-spec-validator, boto3, bravado-core, bravado, neptune\n",
            "Successfully installed GitPython-3.1.43 arrow-1.3.0 boto3-1.34.111 botocore-1.34.111 bravado-11.0.3 bravado-core-6.1.1 fqdn-1.5.1 gitdb-4.0.11 isoduration-20.11.0 jmespath-1.0.1 jsonpointer-2.4 jsonref-1.1.0 monotonic-1.6 neptune-1.10.4 rfc3339-validator-0.1.4 rfc3986-validator-0.1.1 s3transfer-0.10.1 simplejson-3.19.2 smmap-5.0.1 swagger-spec-validator-3.0.3 types-python-dateutil-2.9.0.20240316 uri-template-1.3.0\n"
          ]
        }
      ],
      "source": [
        "# NOTE: you can still run this and it should still work and send the data to my Neptune.ai project,\n",
        "# unfortunately you won't be able to see the graph without my account\n",
        "# TO-DO: add other people to the neptune project, or export back to colab in some way with a CSV output.\n",
        "%pip install -U neptune\n",
        "import neptune\n",
        "from getpass import getpass\n",
        "\n",
        "project=\"dimitri-kachler-workspace/sanity-MNIST\"\n",
        "api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNWQxNDllOS04OGY1LTRjM2EtYTczZi0xNWI0NTRmZTA1OTEifQ==\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### BUG: For whatever reason I can't use Neptune in a function like this, it doesn't log anything\n",
        "def runWithNeptune(setups):\n",
        "  run = neptune.init_run(api_token=api_token, project=project)\n",
        "  all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups)\n",
        "  run.stop()\n",
        "  return all_models,all_training_losses,all_testing_losses,all_accuracies"
      ],
      "metadata": {
        "id": "God805QubYTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q25ZdCrjzVZd"
      },
      "source": [
        "# Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        },
        "id": "TTG9HAmZRlwo",
        "outputId": "c779f638-7029-4abb-81af-e394c795197f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/dimitri-kachler-workspace/sanity-MNIST/e/SAN-129\n",
            "[{'epochs': 5, 'scheme': 'ihtAGD', 'lr': 0.1, 'sparsity': 0.9, 'kappa': 3.0, 'beta': 50.0}, {'epochs': 5, 'scheme': 'vanillaAGD', 'lr': 0.1, 'sparsity': 0.9, 'kappa': 3.0, 'beta': 50.0}, {'epochs': 5, 'scheme': 'ihtSGD', 'lr': 0.1, 'sparsity': 0.9}, {'epochs': 5, 'scheme': 'vanillaSGD', 'lr': 0.1, 'sparsity': 0.9}]\n",
            "iht_AGD\n",
            "iht_AGD\n",
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.305031\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.542139\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.913042\n",
            "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.736972\n",
            "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.673354\n",
            "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.483504\n",
            "\n",
            "Accuracy: 8592/10000 (86%)\n",
            "\n",
            "iht_AGD\n",
            "iht_AGD\n",
            "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.452763\n",
            "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.402066\n",
            "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.438960\n",
            "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.355656\n",
            "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.350633\n",
            "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.350744\n",
            "\n",
            "Accuracy: 9141/10000 (91%)\n",
            "\n",
            "iht_AGD\n",
            "iht_AGD\n",
            "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.338854\n",
            "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.322408\n",
            "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.308437\n",
            "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.270875\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-0eab9579af15>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\"\"\" MAIN CELL \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mall_models\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_training_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_testing_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunMainExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mrun\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-10-4bf3feb3c264>\u001b[0m in \u001b[0;36mrunMainExperiment\u001b[0;34m(setups, epochs)\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mall_accuracies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mall_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_training_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_testing_losses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunOneExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mall_models\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_training_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_testing_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mall_accuracies\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-75bcad2b534a>\u001b[0m in \u001b[0;36mrunOneExperiment\u001b[0;34m(setup, trialNumber)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;31m# Call to run one epoch of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m     \u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_epoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrialNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtraining_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_epoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-bef8ba7f635c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch, trialNumber)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;31m# Backwards Propogation and Calling the Optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;31m#NOTE: Is there going to be a naming conflict with 'params'?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\" MAIN CELL \"\"\"\n",
        "run = neptune.init_run(api_token=api_token, project=project)\n",
        "all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups)\n",
        "run.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF THE BASELINE FRAMEWORK, NEXT SECTION DEDICATED TO EXTENSIONS\n"
      ],
      "metadata": {
        "id": "s-LBtmTsUiSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bias Left Untouched"
      ],
      "metadata": {
        "id": "prSnPcFPUna-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class untouchedIhtAGD(ihtAGD):\n",
        "  def __init__(self,params,sparsity=0.9,kappa=5.0,beta=50.0):\n",
        "    super().__init__(params)\n",
        "    self.methodName = \"untouched_iht_AGD\"\n",
        "    self.alpha = beta / kappa\n",
        "    self.beta = beta\n",
        "    self.kappa = kappa\n",
        "\n",
        "  def sparsify(self):\n",
        "    # TO-DO: remember to remove this zero, it is inconsequential, but still remove it in good practice\n",
        "    concatWeights = torch.zeros((1)).to(device)\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "\n",
        "        #Skip Bias Layers\n",
        "        if len(p.data.shape) > 1:\n",
        "          continue\n",
        "\n",
        "        flatWeights = torch.flatten(p.data)\n",
        "        concatWeights = torch.cat((concatWeights,flatWeights),0)\n",
        "\n",
        "    topK = int(len(concatWeights)*(1-self.sparsity))\n",
        "    vals, bestI = torch.topk(torch.abs(concatWeights),topK,dim=0)\n",
        "    cutoff = vals[-1]\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "\n",
        "        #Skip Bias Layers\n",
        "        if len(p.data.shape) > 1:\n",
        "          continue\n",
        "\n",
        "        p.data[abs(p.data) <= cutoff] = 0.0\n"
      ],
      "metadata": {
        "id": "bcAf_EsRUq0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup_untouched_ihtAGD = {\n",
        "    \"scheme\":\"untouchedIhtAGD\",\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.90,\n",
        "    \"kappa\":10.0,\n",
        "    \"beta\":100.0}\n",
        "setups = [setup_untouched_ihtAGD, setup_ihtAGD]\n",
        "\n",
        "run = neptune.init_run(api_token=api_token, project=project)\n",
        "all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups)\n",
        "run.stop()"
      ],
      "metadata": {
        "id": "lS7mdfZIXedV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search"
      ],
      "metadata": {
        "id": "oxXZNYnHQj9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from os import setgroups\n",
        "\n",
        "def gridSearch(default,variables,values,metric,epochs=1):\n",
        "  \"\"\" Desc: searches in a grid for the best combination of values of arbitrary dimension,\n",
        "        we can check for more than 2 variables at a time, but this can be very costly\n",
        "\n",
        "  default [dictionary]: a dictionary for all the default settings, this is also how one can set the type of algorithm\n",
        "  variables [array[string]]: the settings to change\n",
        "  values [2Darray]: what values to take on\n",
        "  metric [string]: what metric to use for the best value\n",
        "  \"\"\"\n",
        "\n",
        "  # We will not know how to traverse this list easily however\n",
        "  # TO-DO: find a way to organize, or traverse this list\n",
        "  setups = []\n",
        "\n",
        "  # This list has every possible combination of the settings\n",
        "  valuePermutations = list(itertools.product(*values))\n",
        "\n",
        "  for permutation in valuePermutations:\n",
        "    newSetup = default.copy()\n",
        "    for idx,val in enumerate(permutation):\n",
        "\n",
        "      # Adjusts the settings one-by-one\n",
        "      newSetup[variables[idx]] = val\n",
        "\n",
        "    setups.append(newSetup)\n",
        "\n",
        "  print(setups)\n",
        "\n",
        "\n",
        "  all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups,epochs=epochs)\n",
        "\n",
        "  # NEXT: Interchange with a different metric\n",
        "  # TO-DO: try \"highest loss\" over entire dataset using model\n",
        "\n",
        "  # Right now we use the accuracy in after the last epoch\n",
        "  # BUG: is the last epoch at 0 or -1 I need to check\n",
        "  min_accuracies = [accuracies[-1] for accuracies in all_accuracies]\n",
        "  bestSetupIndex = min_accuracies.index(min(min_accuracies))\n",
        "\n",
        "\n",
        "\n",
        "  return setups[bestSetupIndex]"
      ],
      "metadata": {
        "id": "_i4nU5QFQl3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "default = {\n",
        "    \"scheme\":\"vanillaAGD\",\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.90,\n",
        "    \"kappa\":15.0,\n",
        "    \"beta\":10000.0}\n",
        "# We set a big value to see if we overwrite it in the Grid Search\n",
        "\n",
        "gridSearch(default,[\"kappa\",\"beta\"],[[2.0,10.0,100.0],[10.0,100.0,300.0]],\"loss\",5)"
      ],
      "metadata": {
        "id": "cdkn9Z7tHt7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This works! It recognizes it as a class name\n",
        "type(eval(\"ihtAGD\"))"
      ],
      "metadata": {
        "id": "NCz_DpeoYqoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# END OF THE CODE"
      ],
      "metadata": {
        "id": "NPgM-vS5INLT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iIckbVLhIUbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wuXVO80GIUP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Appendix**"
      ],
      "metadata": {
        "id": "5mTrAB1jIWvH"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00N56_990kMg"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u0j_Ugee0oIO"
      },
      "outputs": [],
      "source": [
        "def plotAllAccuracies(all_accuracies):\n",
        "  # TO-DO: make dynamic instead of set labels\n",
        "  sparses = [0.5,0.9,0.95]\n",
        "  for idx,accuracies in enumerate(all_accuracies):\n",
        "    xes = [i for i in range(len(accuracies))]\n",
        "    plt.plot(xes,np.array(accuracies)*100,label=f\"Sparsity: {sparses[idx]}\")\n",
        "  plt.ylim([80,100])\n",
        "  #plt.yscale(\"log\")\n",
        "  plt.title(\"MNIST - Accuracies\")\n",
        "  plt.xlabel(\"Training Iterations\")\n",
        "  plt.ylabel(\"Accuracy (%)\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "plt.figure(figsize=(20,14))\n",
        "plotAllAccuracies(all_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do6orJ90Xqg_"
      },
      "outputs": [],
      "source": [
        "def plotAllLosses(all_training,all_testing):\n",
        "  #print(list_losses)\n",
        "  sparses = [0.5,0.9,0.95]\n",
        "  for idx,train_loss in enumerate(all_training):\n",
        "    xes = [i for i in range(len(train_loss))]\n",
        "    #plt.plot(xes,train_loss,label='Training Loss',color='turquoise',linestyle='dashed')\n",
        "    plt.plot(xes,train_loss,label=f\"Train: {sparses[idx]}\")\n",
        "  for idx,test_loss in enumerate(all_testing):\n",
        "    print(test_loss)\n",
        "    xes = [i for i in range(len(test_loss))]\n",
        "    #plt.plot(xes,test_loss,label=f\"Test: {sparses[idx]}\",linestyle='dashed')\n",
        "  #plt.ylim([0,1.0])\n",
        "  #plt.yscale(\"log\")\n",
        "  plt.title(\"MNIST - FIRST TRIAL\")\n",
        "  plt.xlabel(\"Training Iterations\")\n",
        "  plt.ylabel(\"Negative Log-Likelihood Loss\")\n",
        "  plt.legend(loc=\"upper right\")\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "plt.figure(figsize=(20,14))\n",
        "plotAllLosses(all_training_losses,all_testing_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK4nfrXJNb14"
      },
      "source": [
        "# Saving and Loading Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVY_NTm3OKeX"
      },
      "source": [
        "SOURCE: https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4RpYVFvra3M"
      },
      "outputs": [],
      "source": [
        "def saveModel(model,pathdir):\n",
        "  torch.save(model.state_dict(), pathdir)\n",
        "\n",
        "def loadModel(pathdir,modeltype):\n",
        "  match modeltype:\n",
        "    case \"basicNeuralNet\": model = basicNeuralNet(784,10).to(device)\n",
        "    case \"convNet\": model = convNet().to(device)\n",
        "\n",
        "  model.load_state_dict(torch.load(pathdir))\n",
        "  model.eval()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN1hsUtGTHGO"
      },
      "outputs": [],
      "source": [
        "saveModel(all_models[0],\"testModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONc24AR_Wkrb"
      },
      "outputs": [],
      "source": [
        "tryModel = loadModel(\"testModel\",\"convNet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCWs8hWW1MP8"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5McLfF3cWI_8"
      },
      "source": [
        "Sparsify Interval\n",
        "Base case\n",
        "Fine-Tuning Phase (Freeze weights) , < Sparsify interval\n",
        "Real-time visualization - add trainin loss per batch and test loss, and test accuracy\n",
        "Weights and Biases\n",
        "\n",
        "\n",
        "AC/DC proof 8.1.4,\n",
        "\n",
        "Make proof on board work for large numbers, i.e.! T:(S* times Kappa^2 * some constant factor)\n",
        "Want the damage to be 1 + epsilon\n",
        "\n",
        "make sure you can collect useful information - e.g. things like sparsity\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RB081Ul_Bvyb"
      },
      "source": [
        "# Seaborn Integration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wN38Yguira0o"
      },
      "outputs": [],
      "source": [
        "df2 = pd.DataFrame()\n",
        "df2[\"idp\"] = np.array([1,2,3,4,5])\n",
        "df2[\"dep\"][0] = np.array([3,2,6,4,8])\n",
        "df2[\"dep\"][1] = np.array([2,2,3,1,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ufaxYrprayV"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "\n",
        "sns.lineplot(x=\"idp\", y=\"dep\",data=df2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m9xVP7VjratC"
      },
      "outputs": [],
      "source": [
        "def plotTrainingLosses(data):\n",
        "  sns.lineplot(y=\"training_losses\",data=data)\n",
        "  return None\n",
        "def plotTestingLosses(data):\n",
        "  return None\n",
        "def plotAccuracies(data):\n",
        "  return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fx9FaaEd4IF"
      },
      "source": [
        "# Empirically Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E642kOUidWJT"
      },
      "outputs": [],
      "source": [
        "def testModel(model):\n",
        "  randomExampleInt = np.random.randint(1000)\n",
        "  exampleX = dataset2.data[randomExampleInt].reshape(28, 28)\n",
        "  plt.imshow(exampleX)\n",
        "  print(exampleX.shape)\n",
        "  exampleX = torch.reshape(exampleX, (1, 1,28,28))\n",
        "  predicted = model(torch.tensor(exampleX,dtype=torch.float32).to(device))\n",
        "  print(torch.argmax(predicted))\n",
        "\n",
        "testModel(tryModel)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPw0sSgRcIYdjUBAdpuxmcm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}