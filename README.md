Welcome to the repository for **Iterative Hard Thresholding (IHT)** applied to **Accelerated Gradient Descent (AGD)**. 

# Instructions

## The notebook
Download the *main.ipynb* file for trying out experiments.
Tou can run all of the cells and main.ipynb will automatically import the supporting code found in this repository, as well as some other external imports

## Experiments
You can adjust the variables for each type of optimizer (SGD, AGD, IHT-AGD, etc.) in the 'Setups' section. Furthernore, it is also possible to inherit from these optimizer classes to make new optimizers and customize what variables to track

## Real-Time Tracking
As an MLOps service, this project uses Neptune.ai, the link for which is:
https://app.neptune.ai/o/dimitri-kachler-workspace/org/sanity-MNIST/runs/details?viewId=standard-view&detailsTab=dashboard&dashboardId=9c26ef22-c91b-462a-bddf-babfd83a481f&shortId=SAN-489

## Miscellaneous



