Welcome to the repository for **Iterative Hard Thresholding (IHT)** applied to **Accelerated Gradient Descent (AGD)**. 


<img src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/2560px-PyTorch_logo_black.svg.png" width="128"/>

# IHT-AGD Optimizer :bar_chart: 
This codebase has served as a framework to develop the IHT-AGD algorithm for training sparse deep learning models, however, please note that the optimizer files are outdated. The current best iteration of IHT-AGD are found in the experiment reposiories below.

## Experiments :alembic:
The Experiments for CIFAR10 and MNIST we carried out in this repositiory:
https://github.com/NanoNero1/IHT_AGD_CLUSTER
For CIFAR100, the experiments were carried out in this repository: 
https://github.com/NanoNero1/cifar100_baseline

## Real-Time Tracking :chart_with_upwards_trend:
As an MLOps service, this project uses Neptune.ai, the link for which is:
https://app.neptune.ai/o/dimitri-kachler-workspace/org/sanity-MNIST/runs/details?viewId=standard-view&detailsTab=dashboard&dashboardId=9c26ef22-c91b-462a-bddf-babfd83a481f&shortId=SAN-489

