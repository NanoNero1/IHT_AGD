{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_U2DWonNx69r"
      },
      "source": [
        "### AUTHOR: Dimitri Kachler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEAgjIot_r8h"
      },
      "source": [
        "# Global Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fTxMQlrI_rqG"
      },
      "outputs": [],
      "source": [
        "#User-Dependent Variables\n",
        "layerByLayer = False\n",
        "datasetChoice = \"MNIST\"\n",
        "\n",
        "# -------------- INACTIVE\n",
        "#useNeptune = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okBTEBAuZX54"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4xMEbdjEXUk3"
      },
      "outputs": [],
      "source": [
        "# Neural Networks\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim.optimizer import Optimizer, required\n",
        "import torch\n",
        "\n",
        "# Arrays & Mathematics\n",
        "import math\n",
        "import numpy as np\n",
        "\n",
        "#Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "#System / IO\n",
        "import abc\n",
        "import itertools\n",
        "import importlib\n",
        "\n",
        "#Data Visualization\n",
        "#import seaborn as sns\n",
        "\n",
        "#External Utilities\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eUqt83pkPVh",
        "outputId": "99a0844f-8eee-4cfe-d581-02ac841a2819"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.3.0+cu121\n",
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "# CUDA Check\n",
        "print(torch.__version__)\n",
        "device = (\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nwmhYoV2jqci"
      },
      "outputs": [],
      "source": [
        "# NOTE: you can still run this and it should still work and send the data to my Neptune.ai project,\n",
        "# unfortunately you won't be able to see the graph without my account\n",
        "# Capture makes it so that the cell doesn't output text\n",
        "%%capture\n",
        "#\n",
        "try:\n",
        "    import neptune\n",
        "except ImportError as e:\n",
        "    %pip install -U neptune\n",
        "#import neptune\n",
        "from getpass import getpass\n",
        "\n",
        "project=\"dimitri-kachler-workspace/sanity-MNIST\"\n",
        "api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJlNWQxNDllOS04OGY1LTRjM2EtYTczZi0xNWI0NTRmZTA1OTEifQ==\"\n",
        "#project = neptune.init_project(api_token=api_token, project=project)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bQtl6tN0vWb"
      },
      "source": [
        "# Github Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeWatvCl4aXy",
        "outputId": "810306fd-b0de-4536-bdcb-f919a2e603c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'IHT_AGD' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/NanoNero1/IHT_AGD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPb-BbT41iR2",
        "outputId": "9e919ac8-8cca-4605-fa3b-05335c2b941c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/IHT_AGD\n",
            "remote: Enumerating objects: 11, done.\u001b[K\n",
            "remote: Counting objects: 100% (11/11), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 6 (delta 5), reused 6 (delta 5), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (6/6), 481 bytes | 160.00 KiB/s, done.\n",
            "From https://github.com/NanoNero1/IHT_AGD\n",
            "   428d660..d1b413f  main       -> origin/main\n",
            "Updating 428d660..d1b413f\n",
            "Fast-forward\n",
            " optimizers/ihtSGD.py     | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " optimizers/vanillaAGD.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " optimizers/vanillaSGD.py | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 3 files changed, 3 insertions(+), 3 deletions(-)\n"
          ]
        }
      ],
      "source": [
        "%cd /content/IHT_AGD/\n",
        "!git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Dw1TZCfWVg9p",
        "outputId": "3c2cf1a0-741e-4c7b-a1e7-0c625db23a15"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<Figure size 3000x1400 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Data Collection\n",
        "import IHT_AGD.data_loaders.dataLoaders as dataLoaders\n",
        "datasetChoice = dataLoaders.datasetChoice\n",
        "train_loader = dataLoaders.train_loader\n",
        "test_loader = dataLoaders.test_loader\n",
        "\n",
        "# Just for debugging\n",
        "#import IHT_AGD.architectures.architect\n",
        "#IHT_AGD.architectures.architect.seeVariable\n",
        "\n",
        "# Neural Netwok Architecture\n",
        "from IHT_AGD.architectures.convNets import MNIST_convNet\n",
        "\n",
        "# Taining and Testing Functions\n",
        "from IHT_AGD.modelTrainTest.trainingMetrics import getTestAccuracy,getTestLoss\n",
        "from IHT_AGD.modelTrainTest.trainLoop import train\n",
        "\n",
        "# Optimizers (base, SGD, AGD, IHT, etc.)\n",
        "from IHT_AGD.optimizers.baseOptimizer import myOptimizer\n",
        "from IHT_AGD.optimizers.vanillaSGD import vanillaSGD\n",
        "from IHT_AGD.optimizers.ihtSGD import ihtSGD\n",
        "from IHT_AGD.optimizers.vanillaAGD import vanillaAGD\n",
        "from IHT_AGD.optimizers.ihtAGD import ihtAGD\n",
        "from IHT_AGD.optimizers.nativePytorchSGD import dimitriPytorchSGD\n",
        "\n",
        "# Visualization Functions\n",
        "from IHT_AGD.visualizationGraphs.plotting import plotMetric\n",
        "\n",
        "# Experiment Functions\n",
        "from IHT_AGD.experimentScaffolding.chooseOptimizer import chooseOptimizer\n",
        "from IHT_AGD.experimentScaffolding.chooseOptimizer import fixedChooseOptimizer\n",
        "from IHT_AGD.experimentScaffolding.experimentFuncs import runOneExperiment\n",
        "from IHT_AGD.experimentScaffolding.experimentFuncs import runMainExperiment\n",
        "from IHT_AGD.experimentScaffolding.experimentFuncs import runPipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GVWidDypEprA",
        "outputId": "a0e12338-a188-49d4-deb7-37a72736ed98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1000, 1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "#To know the sizes\n",
        "firstInput, firstTarget = next(iter(train_loader))\n",
        "print(firstInput.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwvZ1FqKdtjt"
      },
      "source": [
        "# Tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9L2J1E7ifJ-7"
      },
      "outputs": [],
      "source": [
        "variablesToTrack = ['sparsity','sparsityBias','lr','iteration','trackSparsity','trackSparsityBias','trackSparsityLinear','testAccuracy','beta']\n",
        "functionsToHelpTrack = ['trackingSparsity']#,'getTestAccuracy']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaMmGdAu32BP"
      },
      "source": [
        "# Setups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PETuCM2Z33cY"
      },
      "outputs": [],
      "source": [
        "# NOTE: I think it might be useful to keep the setups here, at least for now since we change the settings often\n",
        "setup_ihtAGD = {\n",
        "    \"scheme\":\"ihtAGD\" ,\n",
        "    \"sparsity\":0.99,\n",
        "    \"kappa\":5.0,\n",
        "    \"beta\":50.0}\n",
        "\n",
        "setup_vanillaAGD = {\n",
        "    \"scheme\":\"vanillaAGD\",\n",
        "    \"sparsity\":0.950,\n",
        "    \"kappa\":5.0,\n",
        "    \"beta\":50.0,\n",
        "    }\n",
        "\n",
        "setup_ihtSGD = {\n",
        "    \"scheme\":\"ihtSGD\" ,\n",
        "    \"sparsity\":0.950,\n",
        "    \"beta\": 50.0,}\n",
        "\n",
        "setup_vanillaSGD = {\n",
        "    \"scheme\":\"vanillaSGD\",\n",
        "    \"sparsity\":0.9,\n",
        "    \"beta\": 50.0,\n",
        "}\n",
        "\n",
        "setup_pytorchSGD = {\n",
        "    \"scheme\":\"pytorchSGD\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q25ZdCrjzVZd"
      },
      "source": [
        "# Running the Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX9f4erHCDCE",
        "outputId": "a22ddd7c-4488-44f1-ea8d-5b8713de7667"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MNIST\n"
          ]
        }
      ],
      "source": [
        "print(datasetChoice)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "TTG9HAmZRlwo",
        "outputId": "2c4f3e01-a665-49aa-fcfd-5567a9e3e77d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[neptune] [warning] NeptuneWarning: The following monitoring options are disabled by default in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', and 'capture_hardware_metrics'. To enable them, set each parameter to 'True' when initializing the run. The monitoring will continue until you call run.stop() or the kernel stops. Also note: Your source files can only be tracked if you pass the path(s) to the 'source_code' argument. For help, see the Neptune docs: https://docs.neptune.ai/logging/source_code/\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/dimitri-kachler-workspace/sanity-MNIST/e/SAN-488\n",
            "{'functionsToHelpTrack': ['trackingSparsity'], 'variablesToTrack': ['sparsity', 'sparsityBias', 'lr', 'iteration', 'trackSparsity', 'trackSparsityBias', 'trackSparsityLinear', 'testAccuracy', 'beta'], 'device': 'cpu', 'run': <neptune.metadata_containers.run.Run object at 0x77fb828cfc10>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x77fb82a6b580>, 'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x77fb82a6a800>}\n",
            "[{'epochs': 6, 'scheme': 'ihtAGD', 'sparsity': 0.99, 'kappa': 5.0, 'beta': 50.0}]\n",
            "0\n",
            "test fixed chooose\n",
            "{'model': MNIST_convNet(\n",
            "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=4608, out_features=128, bias=True)\n",
            "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
            "), 'scheme': 'ihtAGD', 'sparsity': 0.99, 'kappa': 5.0, 'beta': 50.0, 'functionsToHelpTrack': ['trackingSparsity'], 'variablesToTrack': ['sparsity', 'sparsityBias', 'lr', 'iteration', 'trackSparsity', 'trackSparsityBias', 'trackSparsityLinear', 'testAccuracy', 'beta'], 'device': 'cpu', 'run': <neptune.metadata_containers.run.Run object at 0x77fb828cfc10>, 'test_loader': <torch.utils.data.dataloader.DataLoader object at 0x77fb82a6b580>, 'train_loader': <torch.utils.data.dataloader.DataLoader object at 0x77fb82a6a800>, 'trialNumber': 0}\n",
            "Current Scheme: iht_AGD\n",
            "tensor(2.3036, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 0\n",
            "HowFarAlong: 30 / 40\n",
            "Iteration: 0\n",
            "warmup\n",
            "FIXED IHT-AGD\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[neptune] [warning] NeptuneUnsupportedType: You're attempting to log a type that is not directly supported by Neptune (<class 'list'>).\n",
            "        Convert the value to a supported type, such as a string or float, or use stringify_unsupported(obj)\n",
            "        for dictionaries or collections that contain unsupported values.\n",
            "        For more, see https://docs.neptune.ai/help/value_of_unsupported_type\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303620\n",
            "tensor(2.2929, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 1\n",
            "HowFarAlong: 31 / 40\n",
            "Iteration: 1\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(2.2561, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 2\n",
            "HowFarAlong: 32 / 40\n",
            "Iteration: 2\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(2.1872, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 3\n",
            "HowFarAlong: 33 / 40\n",
            "Iteration: 3\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(2.1102, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 4\n",
            "HowFarAlong: 34 / 40\n",
            "Iteration: 4\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(2.0122, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 5\n",
            "HowFarAlong: 35 / 40\n",
            "Iteration: 5\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(1.9048, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 6\n",
            "HowFarAlong: 36 / 40\n",
            "Iteration: 6\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(1.7893, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 7\n",
            "HowFarAlong: 37 / 40\n",
            "Iteration: 7\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(1.6517, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 8\n",
            "HowFarAlong: 38 / 40\n",
            "Iteration: 8\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(1.5238, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 9\n",
            "HowFarAlong: 39 / 40\n",
            "Iteration: 9\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(1.3830, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 10\n",
            "HowFarAlong: 0 / 40\n",
            "Iteration: 10\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.383039\n",
            "tensor(1.2657, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 11\n",
            "HowFarAlong: 1 / 40\n",
            "Iteration: 11\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(1.1681, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 12\n",
            "HowFarAlong: 2 / 40\n",
            "Iteration: 12\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(1.0404, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 13\n",
            "HowFarAlong: 3 / 40\n",
            "Iteration: 13\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.9699, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 14\n",
            "HowFarAlong: 4 / 40\n",
            "Iteration: 14\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.8992, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 15\n",
            "HowFarAlong: 5 / 40\n",
            "Iteration: 15\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.8182, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 16\n",
            "HowFarAlong: 6 / 40\n",
            "Iteration: 16\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.7942, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 17\n",
            "HowFarAlong: 7 / 40\n",
            "Iteration: 17\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.7174, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 18\n",
            "HowFarAlong: 8 / 40\n",
            "Iteration: 18\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.6653, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 19\n",
            "HowFarAlong: 9 / 40\n",
            "Iteration: 19\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.6755, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 20\n",
            "HowFarAlong: 10 / 40\n",
            "Iteration: 20\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.675504\n",
            "tensor(0.6320, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 21\n",
            "HowFarAlong: 11 / 40\n",
            "Iteration: 21\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.6012, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 22\n",
            "HowFarAlong: 12 / 40\n",
            "Iteration: 22\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.6113, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 23\n",
            "HowFarAlong: 13 / 40\n",
            "Iteration: 23\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.6016, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 24\n",
            "HowFarAlong: 14 / 40\n",
            "Iteration: 24\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.5603, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 25\n",
            "HowFarAlong: 15 / 40\n",
            "Iteration: 25\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.5224, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 26\n",
            "HowFarAlong: 16 / 40\n",
            "Iteration: 26\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.5233, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 27\n",
            "HowFarAlong: 17 / 40\n",
            "Iteration: 27\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.4907, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 28\n",
            "HowFarAlong: 18 / 40\n",
            "Iteration: 28\n",
            "warmup\n",
            "FIXED IHT-AGD\n",
            "tensor(0.4699, grad_fn=<NllLossBackward0>)\n",
            "speed iteration 29\n",
            "HowFarAlong: 19 / 40\n",
            "Iteration: 29\n",
            "warmup\n",
            "FIXED IHT-AGD\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-b30a8236d511>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mrun\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mneptune\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mapi_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m runPipeline(setups,\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mdatasetChoice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"MNIST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/experimentScaffolding/experimentFuncs.py\u001b[0m in \u001b[0;36mrunPipeline\u001b[0;34m(setups, datasetChoice, epochs, trials, **kwargs)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mrunMainExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrialNumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/experimentScaffolding/experimentFuncs.py\u001b[0m in \u001b[0;36mrunMainExperiment\u001b[0;34m(setups, epochs, trialNumber, **kwargs)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrialNumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mall_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunOneExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrialNumber\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrialNumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mall_models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/experimentScaffolding/experimentFuncs.py\u001b[0m in \u001b[0;36mrunOneExperiment\u001b[0;34m(setup, trialNumber, datasetChoice, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Call to run one epoch of training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'device'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loader'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrialNumber\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'run'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/modelTrainTest/trainLoop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model, device, train_loader, optimizer, epoch, trialNumber, test_loader, run)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;31m# Optimization Step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentDataBatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m                             )\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 391\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    392\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/optimizers/ihtAGD.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"speed iteration {self.iteration}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompressOrDecompress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/optimizers/ihtSGD.py\u001b[0m in \u001b[0;36mcompressOrDecompress\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteration\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmupLength\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m       \u001b[0;31m## WARMUP -- PHASE 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhowFarAlong\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m       \u001b[0;31m## FREEZING WEIGHTS -- PHASE 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/optimizers/ihtSGD.py\u001b[0m in \u001b[0;36mwarmup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwarmup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'warmup'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdateWeights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mtruncateAndFreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/optimizers/vanillaAGD.py\u001b[0m in \u001b[0;36mupdateWeights\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;31m# CAREFUL! this changes the parameters for the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNewGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'zt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/IHT_AGD/optimizers/vanillaAGD.py\u001b[0m in \u001b[0;36mgetNewGrad\u001b[0;34m(self, iterate)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mnewOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnewOutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;31m# CHECK: see if this works as intended\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    523\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             )\n\u001b[0;32m--> 525\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    526\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 744\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    745\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    746\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\" MAIN CELL \"\"\"\n",
        "setups = [setup_ihtAGD]#,setup_vanillaSGD]#,setup_ihtAGD]\n",
        "#setups = [setup_pytorchSGD]\n",
        "\n",
        "\n",
        "run = neptune.init_run(api_token=api_token, project=project)\n",
        "runPipeline(setups,\n",
        "            datasetChoice=\"MNIST\",\n",
        "            epochs=6,trials=1,\n",
        "            functionsToHelpTrack=functionsToHelpTrack,\n",
        "            variablesToTrack=variablesToTrack,\n",
        "            device=device,\n",
        "            run=run,\n",
        "            test_loader=test_loader,\n",
        "            train_loader=train_loader)\n",
        "run.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0zfWqas3i7d0"
      },
      "outputs": [],
      "source": [
        "importlib.reload(IHT_AGD.experimentScaffolding.chooseOptimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6lc2z6Wwl6Zm"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFXXFE2tYg-c"
      },
      "outputs": [],
      "source": [
        "# Changing the theme to be more pleasing\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "\n",
        "#project=\"dimitri-kachler-workspace/sanity-MNIST\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3M0HjGxI2uK9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "plotMetric(runID=\"SAN-441\",metricName=\"loss\",methodNames=[\"iht_AGD\"],trials=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKigbdgB2t_d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLYrvuBflFwa"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCxzlRNGlFpJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bSdr-W0lFkq"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPnrL5azlFfC"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-LBtmTsUiSM"
      },
      "source": [
        "# -----------------------------------------------------------------------\n",
        "# END OF THE BASELINE FRAMEWORK, NEXT SECTION DEDICATED TO EXTENSIONS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prSnPcFPUna-"
      },
      "source": [
        "## Bias Left Untouched"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcAf_EsRUq0w"
      },
      "outputs": [],
      "source": [
        "class untouchedIhtAGD(ihtAGD):\n",
        "  def __init__(self,params,sparsity=0.9,kappa=5.0,beta=50.0):\n",
        "    super().__init__(params)\n",
        "    self.methodName = \"untouched_iht_AGD\"\n",
        "    self.alpha = beta / kappa\n",
        "    self.beta = beta\n",
        "    self.kappa = kappa\n",
        "\n",
        "  def sparsify(self):\n",
        "    # TO-DO: remember to remove this zero, it is inconsequential, but still remove it in good practice\n",
        "    concatWeights = torch.zeros((1)).to(device)\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "\n",
        "        #Skip Bias Layers\n",
        "        if len(p.data.shape) < 2:\n",
        "          continue\n",
        "\n",
        "        flatWeights = torch.flatten(p.data)\n",
        "        concatWeights = torch.cat((concatWeights,flatWeights),0)\n",
        "\n",
        "    topK = int(len(concatWeights)*(1-self.sparsity))\n",
        "    vals, bestI = torch.topk(torch.abs(concatWeights),topK,dim=0)\n",
        "    cutoff = vals[-1]\n",
        "    for group in self.param_groups:\n",
        "      for p in group['params']:\n",
        "\n",
        "        #Skip Bias Layers\n",
        "        if len(p.data.shape) < 2:\n",
        "          continue\n",
        "\n",
        "        p.data[abs(p.data) <= cutoff] = 0.0\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lS7mdfZIXedV"
      },
      "outputs": [],
      "source": [
        "setup_untouched_ihtAGD = {\n",
        "    \"scheme\":\"untouchedIhtAGD\",\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.90,\n",
        "    \"kappa\":10.0,\n",
        "    \"beta\":100.0}\n",
        "setups = [setup_untouched_ihtAGD, setup_ihtAGD]\n",
        "\n",
        "run = neptune.init_run(api_token=api_token, project=project)\n",
        "all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups)\n",
        "run.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxXZNYnHQj9V"
      },
      "source": [
        "# Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i4nU5QFQl3j"
      },
      "outputs": [],
      "source": [
        "from os import setgroups\n",
        "\n",
        "def gridSearch(default,variables,values,metric,epochs=1):\n",
        "  \"\"\" Desc: searches in a grid for the best combination of values of arbitrary dimension,\n",
        "        we can check for more than 2 variables at a time, but this can be very costly\n",
        "\n",
        "  default [dictionary]: a dictionary for all the default settings, this is also how one can set the type of algorithm\n",
        "  variables [array[string]]: the settings to change\n",
        "  values [2Darray]: what values to take on\n",
        "  metric [string]: what metric to use for the best value\n",
        "  \"\"\"\n",
        "\n",
        "  # We will not know how to traverse this list easily however\n",
        "  # TO-DO: find a way to organize, or traverse this list\n",
        "  setups = []\n",
        "\n",
        "  # This list has every possible combination of the settings\n",
        "  valuePermutations = list(itertools.product(*values))\n",
        "\n",
        "  for permutation in valuePermutations:\n",
        "    newSetup = default.copy()\n",
        "    for idx,val in enumerate(permutation):\n",
        "\n",
        "      # Adjusts the settings one-by-one\n",
        "      newSetup[variables[idx]] = val\n",
        "\n",
        "    setups.append(newSetup)\n",
        "\n",
        "  print(setups)\n",
        "\n",
        "\n",
        "  all_models,all_training_losses,all_testing_losses,all_accuracies = runMainExperiment(setups,epochs=epochs)\n",
        "\n",
        "  # NEXT: Interchange with a different metric\n",
        "  # TO-DO: try \"highest loss\" over entire dataset using model\n",
        "\n",
        "  # Right now we use the accuracy in after the last epoch\n",
        "  # BUG: is the last epoch at 0 or -1 I need to check\n",
        "  min_accuracies = [accuracies[-1] for accuracies in all_accuracies]\n",
        "  bestSetupIndex = min_accuracies.index(min(min_accuracies))\n",
        "\n",
        "\n",
        "\n",
        "  return setups[bestSetupIndex]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cdkn9Z7tHt7y"
      },
      "outputs": [],
      "source": [
        "default = {\n",
        "    \"scheme\":\"vanillaAGD\",\n",
        "    \"lr\":0.1,\n",
        "    \"sparsity\":0.90,\n",
        "    \"kappa\":15.0,\n",
        "    \"beta\":10000.0}\n",
        "# We set a big value to see if we overwrite it in the Grid Search\n",
        "\n",
        "gridSearch(default,[\"kappa\",\"beta\"],[[2.0,10.0,100.0],[10.0,100.0,300.0]],\"loss\",5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCz_DpeoYqoK"
      },
      "outputs": [],
      "source": [
        "#This works! It recognizes it as a class name\n",
        "type(eval(\"ihtAGD\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mTrAB1jIWvH"
      },
      "source": [
        "# **Appendix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK4nfrXJNb14"
      },
      "source": [
        "# Saving and Loading Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVY_NTm3OKeX"
      },
      "source": [
        "SOURCE: https://pytorch.org/tutorials/beginner/saving_loading_models.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4RpYVFvra3M"
      },
      "outputs": [],
      "source": [
        "def saveModel(model,pathdir):\n",
        "  torch.save(model.state_dict(), pathdir)\n",
        "\n",
        "def loadModel(pathdir,modeltype):\n",
        "  match modeltype:\n",
        "    case \"basicNeuralNet\": model = basicNeuralNet(784,10).to(device)\n",
        "    case \"convNet\": model = convNet().to(device)\n",
        "\n",
        "  model.load_state_dict(torch.load(pathdir))\n",
        "  model.eval()\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RN1hsUtGTHGO"
      },
      "outputs": [],
      "source": [
        "saveModel(all_models[0],\"testModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONc24AR_Wkrb"
      },
      "outputs": [],
      "source": [
        "tryModel = loadModel(\"testModel\",\"convNet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCWs8hWW1MP8"
      },
      "source": [
        "# Notes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5McLfF3cWI_8"
      },
      "source": [
        "Sparsify Interval\n",
        "Base case\n",
        "Fine-Tuning Phase (Freeze weights) , < Sparsify interval\n",
        "Real-time visualization - add trainin loss per batch and test loss, and test accuracy\n",
        "Weights and Biases\n",
        "\n",
        "\n",
        "AC/DC proof 8.1.4,\n",
        "\n",
        "Make proof on board work for large numbers, i.e.! T:(S* times Kappa^2 * some constant factor)\n",
        "Want the damage to be 1 + epsilon\n",
        "\n",
        "make sure you can collect useful information - e.g. things like sparsity\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fx9FaaEd4IF"
      },
      "source": [
        "# Empirically Testing the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E642kOUidWJT"
      },
      "outputs": [],
      "source": [
        "def testModel(model):\n",
        "  randomExampleInt = np.random.randint(1000)\n",
        "  exampleX = dataset2.data[randomExampleInt].reshape(28, 28)\n",
        "  plt.imshow(exampleX)\n",
        "  print(exampleX.shape)\n",
        "  exampleX = torch.reshape(exampleX, (1, 1,28,28))\n",
        "  predicted = model(torch.tensor(exampleX,dtype=torch.float32).to(device))\n",
        "  print(torch.argmax(predicted))\n",
        "\n",
        "testModel(tryModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnermn-gzRHe"
      },
      "source": [
        "# TO DO\n",
        "\n",
        "- check the sparsity of bias persists if we increase sparsity (95% sparsity and 99%)\n",
        "\n",
        "- compare with and without first phase I added\n",
        "\n",
        "- try to see if same spike appears with inserting SGD on decompression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1LdUT0Hy2eal"
      },
      "source": [
        "- Visualization?\n",
        "-  Save Model? - MAYBE USEFUL\n",
        "-  Checkpoints? - YES DO THIS!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV8EnZTx2eXj"
      },
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oxXZNYnHQj9V",
        "BK4nfrXJNb14",
        "RCWs8hWW1MP8",
        "7fx9FaaEd4IF"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
